# -*- coding: utf-8 -*-
"""
åŸºç¤çˆ¬èŸ²é¡åˆ¥å’Œå®¹éŒ¯æ©Ÿåˆ¶
"""

import asyncio
import logging
import time
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass
from enum import Enum
import random

from .encoding_utils import EncodingDetector, create_safe_soup
from .rate_limiter import RateLimiter, get_global_rate_limiter
from .cache_manager import CacheManager

logger = logging.getLogger(__name__)


class ErrorType(Enum):
    """éŒ¯èª¤é¡å‹æšèˆ‰"""
    NETWORK_ERROR = "network_error"
    TIMEOUT_ERROR = "timeout_error"
    ENCODING_ERROR = "encoding_error"
    PARSING_ERROR = "parsing_error"
    RATE_LIMIT_ERROR = "rate_limit_error"
    SERVER_ERROR = "server_error"
    CLIENT_ERROR = "client_error"
    UNKNOWN_ERROR = "unknown_error"


@dataclass
class RetryConfig:
    """é‡è©¦é…ç½®"""
    max_retries: int = 3                    # æœ€å¤§é‡è©¦æ¬¡æ•¸
    base_delay: float = 1.0                 # åŸºç¤å»¶é²(ç§’)
    max_delay: float = 60.0                 # æœ€å¤§å»¶é²(ç§’)
    backoff_factor: float = 2.0             # æŒ‡æ•¸é€€é¿å› å­
    jitter: bool = True                     # æ·»åŠ éš¨æ©ŸæŠ–å‹•
    retry_on_errors: List[ErrorType] = None # éœ€è¦é‡è©¦çš„éŒ¯èª¤é¡å‹
    
    def __post_init__(self):
        if self.retry_on_errors is None:
            self.retry_on_errors = [
                ErrorType.NETWORK_ERROR,
                ErrorType.TIMEOUT_ERROR,
                ErrorType.SERVER_ERROR
            ]


@dataclass
class HealthCheckConfig:
    """å¥åº·æª¢æŸ¥é…ç½®"""
    check_interval: float = 300.0           # æª¢æŸ¥é–“éš”(ç§’)
    timeout: float = 10.0                   # è¶…æ™‚æ™‚é–“(ç§’)
    failure_threshold: int = 3              # å¤±æ•—é–¾å€¼
    recovery_threshold: int = 2             # æ¢å¾©é–¾å€¼
    enable_auto_recovery: bool = True       # å•Ÿç”¨è‡ªå‹•æ¢å¾©


class ScrapingException(Exception):
    """çˆ¬èŸ²å°ˆç”¨ç•°å¸¸é¡"""
    
    def __init__(self, message: str, error_type: ErrorType, url: str = None, status_code: int = None):
        super().__init__(message)
        self.error_type = error_type
        self.url = url
        self.status_code = status_code
        self.timestamp = time.time()


class RetryManager:
    """é‡è©¦ç®¡ç†å™¨"""
    
    def __init__(self, config: RetryConfig = None):
        self.config = config or RetryConfig()
        self.stats = {
            'total_attempts': 0,
            'successful_retries': 0,
            'failed_retries': 0,
            'retry_reasons': {}
        }
    
    def should_retry(self, error: Exception, attempt: int) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²é‡è©¦"""
        if attempt >= self.config.max_retries:
            return False
        
        if isinstance(error, ScrapingException):
            return error.error_type in self.config.retry_on_errors
        
        # å…¶ä»–é¡å‹çš„éŒ¯èª¤ä¹Ÿå¯ä»¥é‡è©¦
        return True
    
    def calculate_delay(self, attempt: int) -> float:
        """è¨ˆç®—é‡è©¦å»¶é²"""
        delay = self.config.base_delay * (self.config.backoff_factor ** attempt)
        delay = min(delay, self.config.max_delay)
        
        if self.config.jitter:
            # æ·»åŠ Â±25%çš„éš¨æ©ŸæŠ–å‹•
            jitter_range = delay * 0.25
            delay += random.uniform(-jitter_range, jitter_range)
        
        return max(delay, 0.1)  # æœ€å°å»¶é²0.1ç§’
    
    async def retry_async(self, func: Callable, *args, **kwargs) -> Any:
        """éåŒæ­¥é‡è©¦åŸ·è¡Œ"""
        last_exception = None
        
        for attempt in range(self.config.max_retries + 1):
            self.stats['total_attempts'] += 1
            
            try:
                result = await func(*args, **kwargs)
                
                if attempt > 0:
                    self.stats['successful_retries'] += 1
                    logger.info(f"âœ… é‡è©¦æˆåŠŸ (ç¬¬ {attempt + 1} æ¬¡å˜—è©¦)")
                
                return result
                
            except Exception as e:
                last_exception = e
                
                # è¨˜éŒ„éŒ¯èª¤é¡å‹çµ±è¨ˆ
                error_type = e.error_type if isinstance(e, ScrapingException) else ErrorType.UNKNOWN_ERROR
                if error_type not in self.stats['retry_reasons']:
                    self.stats['retry_reasons'][error_type] = 0
                self.stats['retry_reasons'][error_type] += 1
                
                # åˆ¤æ–·æ˜¯å¦é‡è©¦
                if not self.should_retry(e, attempt):
                    self.stats['failed_retries'] += 1
                    logger.error(f"âŒ é‡è©¦å¤±æ•—ï¼Œä¸å†é‡è©¦: {e}")
                    break
                
                if attempt < self.config.max_retries:
                    delay = self.calculate_delay(attempt)
                    logger.warning(f"âš ï¸ ç¬¬ {attempt + 1} æ¬¡å˜—è©¦å¤±æ•—: {e}")
                    logger.info(f"â³ ç­‰å¾… {delay:.2f} ç§’å¾Œé‡è©¦...")
                    await asyncio.sleep(delay)
        
        # æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—äº†
        if last_exception:
            raise last_exception
        else:
            raise ScrapingException("æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—", ErrorType.UNKNOWN_ERROR)
    
    def retry_sync(self, func: Callable, *args, **kwargs) -> Any:
        """åŒæ­¥é‡è©¦åŸ·è¡Œ"""
        last_exception = None
        
        for attempt in range(self.config.max_retries + 1):
            self.stats['total_attempts'] += 1
            
            try:
                result = func(*args, **kwargs)
                
                if attempt > 0:
                    self.stats['successful_retries'] += 1
                    logger.info(f"âœ… é‡è©¦æˆåŠŸ (ç¬¬ {attempt + 1} æ¬¡å˜—è©¦)")
                
                return result
                
            except Exception as e:
                last_exception = e
                
                # è¨˜éŒ„éŒ¯èª¤é¡å‹çµ±è¨ˆ
                error_type = e.error_type if isinstance(e, ScrapingException) else ErrorType.UNKNOWN_ERROR
                if error_type not in self.stats['retry_reasons']:
                    self.stats['retry_reasons'][error_type] = 0
                self.stats['retry_reasons'][error_type] += 1
                
                # åˆ¤æ–·æ˜¯å¦é‡è©¦
                if not self.should_retry(e, attempt):
                    self.stats['failed_retries'] += 1
                    logger.error(f"âŒ é‡è©¦å¤±æ•—ï¼Œä¸å†é‡è©¦: {e}")
                    break
                
                if attempt < self.config.max_retries:
                    delay = self.calculate_delay(attempt)
                    logger.warning(f"âš ï¸ ç¬¬ {attempt + 1} æ¬¡å˜—è©¦å¤±æ•—: {e}")
                    logger.info(f"â³ ç­‰å¾… {delay:.2f} ç§’å¾Œé‡è©¦...")
                    time.sleep(delay)
        
        # æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—äº†
        if last_exception:
            raise last_exception
        else:
            raise ScrapingException("æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—", ErrorType.UNKNOWN_ERROR)
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–é‡è©¦çµ±è¨ˆ"""
        total = self.stats['total_attempts']
        success_rate = (self.stats['successful_retries'] / total * 100) if total > 0 else 0
        
        return {
            **self.stats,
            'success_rate': f"{success_rate:.1f}%",
            'config': {
                'max_retries': self.config.max_retries,
                'base_delay': self.config.base_delay,
                'backoff_factor': self.config.backoff_factor
            }
        }


class HealthChecker:
    """å¥åº·æª¢æŸ¥å™¨"""
    
    def __init__(self, config: HealthCheckConfig = None):
        self.config = config or HealthCheckConfig()
        self.domain_health = {}  # åŸŸåå¥åº·ç‹€æ…‹
        self.lock = asyncio.Lock()
        
        # å•Ÿå‹•å¥åº·æª¢æŸ¥ä»»å‹™
        if self.config.enable_auto_recovery:
            self._start_health_check_task()
    
    async def check_domain_health(self, domain: str) -> bool:
        """æª¢æŸ¥å–®å€‹åŸŸåçš„å¥åº·ç‹€æ…‹"""
        try:
            import aiohttp
            
            # æ§‹é€ å¥åº·æª¢æŸ¥URL
            check_url = f"https://{domain}/"
            
            timeout = aiohttp.ClientTimeout(total=self.config.timeout)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(check_url) as response:
                    return response.status < 500  # 5xxéŒ¯èª¤è¦–ç‚ºä¸å¥åº·
                    
        except Exception as e:
            logger.debug(f"åŸŸåå¥åº·æª¢æŸ¥å¤±æ•— {domain}: {e}")
            return False
    
    async def update_domain_health(self, domain: str, is_healthy: bool):
        """æ›´æ–°åŸŸåå¥åº·ç‹€æ…‹"""
        async with self.lock:
            if domain not in self.domain_health:
                self.domain_health[domain] = {
                    'healthy': True,
                    'consecutive_failures': 0,
                    'consecutive_successes': 0,
                    'last_check': time.time(),
                    'total_checks': 0,
                    'total_failures': 0
                }
            
            health_info = self.domain_health[domain]
            health_info['last_check'] = time.time()
            health_info['total_checks'] += 1
            
            if is_healthy:
                health_info['consecutive_successes'] += 1
                health_info['consecutive_failures'] = 0
                
                # æ¢å¾©å¥åº·ç‹€æ…‹
                if (not health_info['healthy'] and 
                    health_info['consecutive_successes'] >= self.config.recovery_threshold):
                    health_info['healthy'] = True
                    logger.info(f"âœ… åŸŸå {domain} å·²æ¢å¾©å¥åº·")
            else:
                health_info['consecutive_failures'] += 1
                health_info['consecutive_successes'] = 0
                health_info['total_failures'] += 1
                
                # æ¨™è¨˜ç‚ºä¸å¥åº·
                if (health_info['healthy'] and 
                    health_info['consecutive_failures'] >= self.config.failure_threshold):
                    health_info['healthy'] = False
                    logger.warning(f"âš ï¸ åŸŸå {domain} è¢«æ¨™è¨˜ç‚ºä¸å¥åº·")
    
    def is_domain_healthy(self, domain: str) -> bool:
        """æª¢æŸ¥åŸŸåæ˜¯å¦å¥åº·"""
        if domain not in self.domain_health:
            return True  # æœªçŸ¥åŸŸåé è¨­ç‚ºå¥åº·
        return self.domain_health[domain]['healthy']
    
    def _start_health_check_task(self):
        """å•Ÿå‹•èƒŒæ™¯å¥åº·æª¢æŸ¥ä»»å‹™"""
        async def health_check_worker():
            while True:
                try:
                    await asyncio.sleep(self.config.check_interval)
                    
                    # æª¢æŸ¥æ‰€æœ‰å·²çŸ¥åŸŸå
                    for domain in list(self.domain_health.keys()):
                        is_healthy = await self.check_domain_health(domain)
                        await self.update_domain_health(domain, is_healthy)
                        
                except Exception as e:
                    logger.error(f"å¥åº·æª¢æŸ¥ä»»å‹™å¤±æ•—: {e}")
        
        # åœ¨èƒŒæ™¯åŸ·è¡Œ
        asyncio.create_task(health_check_worker())
        logger.info(f"ğŸ¥ å¥åº·æª¢æŸ¥ä»»å‹™å·²å•Ÿå‹• (é–“éš”: {self.config.check_interval}ç§’)")
    
    def get_health_report(self) -> Dict[str, Any]:
        """ç²å–å¥åº·å ±å‘Š"""
        healthy_domains = []
        unhealthy_domains = []
        
        for domain, info in self.domain_health.items():
            if info['healthy']:
                healthy_domains.append(domain)
            else:
                unhealthy_domains.append(domain)
        
        total_domains = len(self.domain_health)
        health_rate = (len(healthy_domains) / total_domains * 100) if total_domains > 0 else 100
        
        return {
            'total_domains': total_domains,
            'healthy_domains': healthy_domains,
            'unhealthy_domains': unhealthy_domains,
            'health_rate': f"{health_rate:.1f}%",
            'domain_details': self.domain_health
        }


class BaseScraper(ABC):
    """åŸºç¤çˆ¬èŸ²æŠ½è±¡é¡"""
    
    def __init__(self, 
                 encoding_detector: EncodingDetector = None,
                 rate_limiter: RateLimiter = None,
                 cache_manager: CacheManager = None,
                 retry_manager: RetryManager = None,
                 health_checker: HealthChecker = None):
        
        self.encoding_detector = encoding_detector or EncodingDetector()
        self.rate_limiter = rate_limiter or get_global_rate_limiter()
        self.cache_manager = cache_manager or CacheManager()
        self.retry_manager = retry_manager or RetryManager()
        self.health_checker = health_checker or HealthChecker()
        
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'cache_hits': 0,
            'retry_attempts': 0
        }
    
    @abstractmethod
    async def scrape_url(self, url: str) -> Dict[str, Any]:
        """æŠ½è±¡æ–¹æ³•ï¼šçˆ¬å–å–®å€‹URL"""
        pass
    
    @abstractmethod
    def parse_content(self, content: str, url: str) -> Dict[str, Any]:
        """æŠ½è±¡æ–¹æ³•ï¼šè§£æå…§å®¹"""
        pass
    
    async def safe_scrape(self, url: str) -> Dict[str, Any]:
        """å®‰å…¨çˆ¬å–ï¼ˆåŒ…å«æ‰€æœ‰ä¿è­·æ©Ÿåˆ¶ï¼‰"""
        domain = url.split('//')[-1].split('/')[0]
        
        # æª¢æŸ¥åŸŸåå¥åº·ç‹€æ…‹
        if not self.health_checker.is_domain_healthy(domain):
            raise ScrapingException(f"åŸŸå {domain} ç•¶å‰ä¸å¥åº·", ErrorType.SERVER_ERROR, url)
        
        # ä½¿ç”¨é‡è©¦ç®¡ç†å™¨
        try:
            result = await self.retry_manager.retry_async(self._scrape_with_protection, url)
            
            # æ›´æ–°å¥åº·ç‹€æ…‹
            await self.health_checker.update_domain_health(domain, True)
            
            return result
            
        except Exception as e:
            # æ›´æ–°å¥åº·ç‹€æ…‹
            await self.health_checker.update_domain_health(domain, False)
            raise e
    
    async def _scrape_with_protection(self, url: str) -> Dict[str, Any]:
        """å¸¶ä¿è­·æ©Ÿåˆ¶çš„çˆ¬å–"""
        # é »ç‡æ§åˆ¶
        await self.rate_limiter.wait_if_needed_async(url)
        
        # æª¢æŸ¥å¿«å–
        cached_result = await self.cache_manager.get_async(url)
        if cached_result:
            self.stats['cache_hits'] += 1
            return cached_result
        
        # åŸ·è¡Œå¯¦éš›çˆ¬å–
        self.stats['total_requests'] += 1
        
        try:
            result = await self.scrape_url(url)
            
            # è¨˜éŒ„æˆåŠŸ
            self.rate_limiter.record_request(url, True, 0.0)
            self.stats['successful_requests'] += 1
            
            # å„²å­˜åˆ°å¿«å–
            await self.cache_manager.set_async(url, result)
            
            return result
            
        except Exception as e:
            # è¨˜éŒ„å¤±æ•—
            self.rate_limiter.record_request(url, False, 0.0)
            self.stats['failed_requests'] += 1
            raise e
    
    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """ç²å–ç¶œåˆçµ±è¨ˆè³‡è¨Š"""
        return {
            'scraper_stats': self.stats,
            'encoding_stats': self.encoding_detector.get_stats(),
            'rate_limiter_stats': self.rate_limiter.get_stats(),
            'cache_stats': self.cache_manager.get_stats(),
            'retry_stats': self.retry_manager.get_stats(),
            'health_stats': self.health_checker.get_health_report()
        }