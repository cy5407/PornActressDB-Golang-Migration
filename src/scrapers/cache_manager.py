# -*- coding: utf-8 -*-
"""
æ™ºæ…§å¿«å–ç®¡ç†æ¨¡çµ„
æä¾›é«˜æ•ˆçš„å¤šå±¤ç´šå¿«å–æ©Ÿåˆ¶
"""

import asyncio
import sqlite3
import json
import hashlib
import time
import logging
import threading
from datetime import datetime, timedelta
from typing import Any, Optional, Dict, List, Tuple
from pathlib import Path
from dataclasses import dataclass, asdict
import pickle
import gzip

logger = logging.getLogger(__name__)


@dataclass
class CacheConfig:
    """å¿«å–é…ç½®é¡"""
    cache_dir: str = "cache"                    # å¿«å–ç›®éŒ„
    db_file: str = "cache_index.db"             # SQLiteç´¢å¼•æª”æ¡ˆ
    default_ttl_hours: int = 24                 # é è¨­TTL(å°æ™‚)
    max_memory_entries: int = 1000              # è¨˜æ†¶é«”å¿«å–æœ€å¤§æ¢ç›®æ•¸
    enable_compression: bool = True             # å•Ÿç”¨å£“ç¸®
    enable_memory_cache: bool = True            # å•Ÿç”¨è¨˜æ†¶é«”å¿«å–
    enable_disk_cache: bool = True              # å•Ÿç”¨ç£ç¢Ÿå¿«å–
    cleanup_interval_hours: int = 6             # æ¸…ç†é–“éš”(å°æ™‚)
    max_file_size_mb: int = 10                  # å–®æª”æœ€å¤§å¤§å°(MB)


@dataclass
class CacheEntry:
    """å¿«å–æ¢ç›®"""
    key: str
    value: Any
    created_at: float
    ttl_seconds: int
    access_count: int = 0
    last_accessed: float = 0.0
    compressed: bool = False
    size_bytes: int = 0


class CacheManager:
    """å¤šå±¤ç´šæ™ºæ…§å¿«å–ç®¡ç†å™¨"""
    
    def __init__(self, config: CacheConfig = None):
        self.config = config or CacheConfig()
        self.cache_dir = Path(self.config.cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # è¨˜æ†¶é«”å¿«å–
        self.memory_cache: Dict[str, CacheEntry] = {}
        self.memory_lock = threading.RLock()
        
        # è³‡æ–™åº«é€£ç·š
        self.db_path = self.cache_dir / self.config.db_file
        self._init_database()
        
        # çµ±è¨ˆè³‡è¨Š
        self.stats = {
            'memory_hits': 0,
            'disk_hits': 0,
            'misses': 0,
            'sets': 0,
            'deletes': 0,
            'cleanups': 0,
            'total_size_mb': 0.0
        }
        
        # å•Ÿå‹•èƒŒæ™¯æ¸…ç†ä»»å‹™
        self._start_cleanup_task()
        
        logger.info(f"ğŸ’¾ å¿«å–ç®¡ç†å™¨å·²åˆå§‹åŒ– - ç›®éŒ„: {self.cache_dir}")
    
    def _init_database(self):
        """åˆå§‹åŒ–SQLiteç´¢å¼•è³‡æ–™åº«"""
        try:
            with sqlite3.connect(str(self.db_path)) as conn:
                conn.execute('''
                    CREATE TABLE IF NOT EXISTS cache_index (
                        key TEXT PRIMARY KEY,
                        file_path TEXT,
                        created_at REAL,
                        ttl_seconds INTEGER,
                        access_count INTEGER DEFAULT 0,
                        last_accessed REAL,
                        compressed BOOLEAN DEFAULT 0,
                        size_bytes INTEGER DEFAULT 0
                    )
                ''')
                
                # å‰µå»ºç´¢å¼•
                conn.execute('CREATE INDEX IF NOT EXISTS idx_created_at ON cache_index(created_at)')
                conn.execute('CREATE INDEX IF NOT EXISTS idx_last_accessed ON cache_index(last_accessed)')
                
                conn.commit()
                logger.debug("ğŸ“Š å¿«å–ç´¢å¼•è³‡æ–™åº«å·²åˆå§‹åŒ–")
                
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¿«å–è³‡æ–™åº«å¤±æ•—: {e}")
    
    def _generate_cache_key(self, key: str) -> str:
        """ç”Ÿæˆå¿«å–éµå€¼"""
        return hashlib.sha256(key.encode('utf-8')).hexdigest()
    
    def _get_file_path(self, cache_key: str) -> Path:
        """ç²å–å¿«å–æª”æ¡ˆè·¯å¾‘"""
        # ä½¿ç”¨å…©å±¤ç›®éŒ„çµæ§‹é¿å…å–®ç›®éŒ„æª”æ¡ˆéå¤š
        dir1 = cache_key[:2]
        dir2 = cache_key[2:4]
        cache_file_dir = self.cache_dir / dir1 / dir2
        cache_file_dir.mkdir(parents=True, exist_ok=True)
        return cache_file_dir / f"{cache_key}.cache"
    
    def _serialize_value(self, value: Any) -> Tuple[bytes, bool]:
        """åºåˆ—åŒ–å€¼ä¸¦é¸æ“‡æ€§å£“ç¸®"""
        try:
            # åºåˆ—åŒ–
            serialized = pickle.dumps(value)
            
            # æ±ºå®šæ˜¯å¦å£“ç¸®
            should_compress = (
                self.config.enable_compression and 
                len(serialized) > 1024  # è¶…é1KBæ‰å£“ç¸®
            )
            
            if should_compress:
                compressed_data = gzip.compress(serialized)
                # åªæœ‰åœ¨å£“ç¸®æœ‰æ•ˆæœæ™‚æ‰ä½¿ç”¨
                if len(compressed_data) < len(serialized) * 0.9:
                    return compressed_data, True
            
            return serialized, False
            
        except Exception as e:
            logger.error(f"åºåˆ—åŒ–å€¼å¤±æ•—: {e}")
            return b'', False
    
    def _deserialize_value(self, data: bytes, compressed: bool) -> Any:
        """ååºåˆ—åŒ–å€¼"""
        try:
            if compressed:
                data = gzip.decompress(data)
            return pickle.loads(data)
        except Exception as e:
            logger.error(f"ååºåˆ—åŒ–å€¼å¤±æ•—: {e}")
            return None
    
    def _is_expired(self, created_at: float, ttl_seconds: int) -> bool:
        """æª¢æŸ¥æ˜¯å¦éæœŸ"""
        return time.time() - created_at > ttl_seconds
    
    def set(self, key: str, value: Any, ttl_hours: Optional[int] = None) -> bool:
        """è¨­ç½®å¿«å–å€¼"""
        ttl_seconds = (ttl_hours or self.config.default_ttl_hours) * 3600
        cache_key = self._generate_cache_key(key)
        current_time = time.time()
        
        try:
            # åºåˆ—åŒ–å’Œå£“ç¸®
            serialized_data, compressed = self._serialize_value(value)
            size_bytes = len(serialized_data)
            
            # æª¢æŸ¥æª”æ¡ˆå¤§å°é™åˆ¶
            max_size_bytes = self.config.max_file_size_mb * 1024 * 1024
            if size_bytes > max_size_bytes:
                logger.warning(f"å¿«å–å€¼éå¤§ ({size_bytes/1024/1024:.1f}MB)ï¼Œè·³éå¿«å–")
                return False
            
            # å»ºç«‹å¿«å–æ¢ç›®
            entry = CacheEntry(
                key=cache_key,
                value=value,
                created_at=current_time,
                ttl_seconds=ttl_seconds,
                last_accessed=current_time,
                compressed=compressed,
                size_bytes=size_bytes
            )
            
            # è¨­ç½®è¨˜æ†¶é«”å¿«å–
            if self.config.enable_memory_cache:
                with self.memory_lock:
                    self.memory_cache[cache_key] = entry
                    self._cleanup_memory_cache()
            
            # è¨­ç½®ç£ç¢Ÿå¿«å–
            if self.config.enable_disk_cache:
                file_path = self._get_file_path(cache_key)
                
                # å¯«å…¥æª”æ¡ˆ
                with open(file_path, 'wb') as f:
                    f.write(serialized_data)
                
                # æ›´æ–°ç´¢å¼•
                with sqlite3.connect(str(self.db_path)) as conn:
                    conn.execute('''
                        INSERT OR REPLACE INTO cache_index 
                        (key, file_path, created_at, ttl_seconds, last_accessed, compressed, size_bytes)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        cache_key, str(file_path), current_time, ttl_seconds,
                        current_time, compressed, size_bytes
                    ))
                    conn.commit()
            
            self.stats['sets'] += 1
            logger.debug(f"ğŸ’¾ å·²å¿«å–: {key} ({size_bytes} bytes)")
            return True
            
        except Exception as e:
            logger.error(f"è¨­ç½®å¿«å–å¤±æ•—: {e}")
            return False
    
    def get(self, key: str) -> Optional[Any]:
        """ç²å–å¿«å–å€¼"""
        cache_key = self._generate_cache_key(key)
        current_time = time.time()
        
        # å˜—è©¦è¨˜æ†¶é«”å¿«å–
        if self.config.enable_memory_cache:
            with self.memory_lock:
                if cache_key in self.memory_cache:
                    entry = self.memory_cache[cache_key]
                    
                    # æª¢æŸ¥æ˜¯å¦éæœŸ
                    if not self._is_expired(entry.created_at, entry.ttl_seconds):
                        entry.access_count += 1
                        entry.last_accessed = current_time
                        self.stats['memory_hits'] += 1
                        logger.debug(f"ğŸ“‹ è¨˜æ†¶é«”å¿«å–å‘½ä¸­: {key}")
                        return entry.value
                    else:
                        # éæœŸï¼Œå¾è¨˜æ†¶é«”ç§»é™¤
                        del self.memory_cache[cache_key]
        
        # å˜—è©¦ç£ç¢Ÿå¿«å–
        if self.config.enable_disk_cache:
            try:
                with sqlite3.connect(str(self.db_path)) as conn:
                    cursor = conn.execute('''
                        SELECT file_path, created_at, ttl_seconds, compressed, access_count
                        FROM cache_index WHERE key = ?
                    ''', (cache_key,))
                    
                    result = cursor.fetchone()
                    if result:
                        file_path, created_at, ttl_seconds, compressed, access_count = result
                        
                        # æª¢æŸ¥æ˜¯å¦éæœŸ
                        if not self._is_expired(created_at, ttl_seconds):
                            file_path_obj = Path(file_path)
                            
                            if file_path_obj.exists():
                                # è®€å–æª”æ¡ˆ
                                with open(file_path_obj, 'rb') as f:
                                    data = f.read()
                                
                                # ååºåˆ—åŒ–
                                value = self._deserialize_value(data, compressed)
                                
                                if value is not None:
                                    # æ›´æ–°è¨ªå•çµ±è¨ˆ
                                    conn.execute('''
                                        UPDATE cache_index 
                                        SET access_count = access_count + 1, last_accessed = ?
                                        WHERE key = ?
                                    ''', (current_time, cache_key))
                                    conn.commit()
                                    
                                    # è¼‰å…¥åˆ°è¨˜æ†¶é«”å¿«å–
                                    if self.config.enable_memory_cache:
                                        with self.memory_lock:
                                            entry = CacheEntry(
                                                key=cache_key,
                                                value=value,
                                                created_at=created_at,
                                                ttl_seconds=ttl_seconds,
                                                access_count=access_count + 1,
                                                last_accessed=current_time,
                                                compressed=compressed,
                                                size_bytes=len(data)
                                            )
                                            self.memory_cache[cache_key] = entry
                                    
                                    self.stats['disk_hits'] += 1
                                    logger.debug(f"ğŸ’¿ ç£ç¢Ÿå¿«å–å‘½ä¸­: {key}")
                                    return value
                        else:
                            # éæœŸï¼Œæ¸…ç†
                            self._delete_cache_entry(cache_key, file_path)
                            
            except Exception as e:
                logger.error(f"è®€å–ç£ç¢Ÿå¿«å–å¤±æ•—: {e}")
        
        self.stats['misses'] += 1
        logger.debug(f"âŒ å¿«å–æœªå‘½ä¸­: {key}")
        return None
    
    def delete(self, key: str) -> bool:
        """åˆªé™¤å¿«å–æ¢ç›®"""
        cache_key = self._generate_cache_key(key)
        
        try:
            # å¾è¨˜æ†¶é«”ç§»é™¤
            if self.config.enable_memory_cache:
                with self.memory_lock:
                    self.memory_cache.pop(cache_key, None)
            
            # å¾ç£ç¢Ÿç§»é™¤
            if self.config.enable_disk_cache:
                with sqlite3.connect(str(self.db_path)) as conn:
                    cursor = conn.execute('SELECT file_path FROM cache_index WHERE key = ?', (cache_key,))
                    result = cursor.fetchone()
                    
                    if result:
                        file_path = result[0]
                        self._delete_cache_entry(cache_key, file_path)
            
            self.stats['deletes'] += 1
            logger.debug(f"ğŸ—‘ï¸ å·²åˆªé™¤å¿«å–: {key}")
            return True
            
        except Exception as e:
            logger.error(f"åˆªé™¤å¿«å–å¤±æ•—: {e}")
            return False
    
    def _delete_cache_entry(self, cache_key: str, file_path: str):
        """åˆªé™¤å¿«å–æ¢ç›®ï¼ˆæª”æ¡ˆå’Œç´¢å¼•ï¼‰"""
        try:
            # åˆªé™¤æª”æ¡ˆ
            file_path_obj = Path(file_path)
            if file_path_obj.exists():
                file_path_obj.unlink()
            
            # å¾ç´¢å¼•ç§»é™¤
            with sqlite3.connect(str(self.db_path)) as conn:
                conn.execute('DELETE FROM cache_index WHERE key = ?', (cache_key,))
                conn.commit()
                
        except Exception as e:
            logger.error(f"åˆªé™¤å¿«å–æ¢ç›®å¤±æ•—: {e}")
    
    def _cleanup_memory_cache(self):
        """æ¸…ç†è¨˜æ†¶é«”å¿«å–ï¼ˆLRUç­–ç•¥ï¼‰"""
        if len(self.memory_cache) <= self.config.max_memory_entries:
            return
        
        # æŒ‰æœ€å¾Œè¨ªå•æ™‚é–“æ’åºï¼Œç§»é™¤æœ€èˆŠçš„æ¢ç›®
        sorted_entries = sorted(
            self.memory_cache.items(),
            key=lambda x: x[1].last_accessed
        )
        
        # ç§»é™¤è¶…å‡ºé™åˆ¶çš„æ¢ç›®
        remove_count = len(self.memory_cache) - self.config.max_memory_entries
        for i in range(remove_count):
            cache_key, _ = sorted_entries[i]
            del self.memory_cache[cache_key]
        
        logger.debug(f"ğŸ§¹ è¨˜æ†¶é«”å¿«å–æ¸…ç†: ç§»é™¤ {remove_count} å€‹æ¢ç›®")
    
    def _cleanup_expired_cache(self):
        """æ¸…ç†éæœŸå¿«å–"""
        try:
            current_time = time.time()
            
            # æ¸…ç†è¨˜æ†¶é«”å¿«å–
            if self.config.enable_memory_cache:
                with self.memory_lock:
                    expired_keys = [
                        key for key, entry in self.memory_cache.items()
                        if self._is_expired(entry.created_at, entry.ttl_seconds)
                    ]
                    
                    for key in expired_keys:
                        del self.memory_cache[key]
                    
                    if expired_keys:
                        logger.info(f"ğŸ§¹ è¨˜æ†¶é«”å¿«å–æ¸…ç†: {len(expired_keys)} å€‹éæœŸæ¢ç›®")
            
            # æ¸…ç†ç£ç¢Ÿå¿«å–
            if self.config.enable_disk_cache:
                with sqlite3.connect(str(self.db_path)) as conn:
                    # æŸ¥æ‰¾éæœŸæ¢ç›®
                    cursor = conn.execute('''
                        SELECT key, file_path FROM cache_index 
                        WHERE ? - created_at > ttl_seconds
                    ''', (current_time,))
                    
                    expired_entries = cursor.fetchall()
                    
                    # åˆªé™¤éæœŸæ¢ç›®
                    for cache_key, file_path in expired_entries:
                        self._delete_cache_entry(cache_key, file_path)
                    
                    if expired_entries:
                        logger.info(f"ğŸ§¹ ç£ç¢Ÿå¿«å–æ¸…ç†: {len(expired_entries)} å€‹éæœŸæ¢ç›®")
            
            self.stats['cleanups'] += 1
            
        except Exception as e:
            logger.error(f"æ¸…ç†éæœŸå¿«å–å¤±æ•—: {e}")
    
    def _start_cleanup_task(self):
        """å•Ÿå‹•èƒŒæ™¯æ¸…ç†ä»»å‹™"""
        def cleanup_worker():
            while True:
                try:
                    time.sleep(self.config.cleanup_interval_hours * 3600)
                    self._cleanup_expired_cache()
                except Exception as e:
                    logger.error(f"èƒŒæ™¯æ¸…ç†ä»»å‹™å¤±æ•—: {e}")
        
        cleanup_thread = threading.Thread(target=cleanup_worker, daemon=True)
        cleanup_thread.start()
        logger.info(f"ğŸ§¹ èƒŒæ™¯æ¸…ç†ä»»å‹™å·²å•Ÿå‹• (é–“éš”: {self.config.cleanup_interval_hours}å°æ™‚)")
    
    def clear_cache(self):
        """æ¸…ç©ºæ‰€æœ‰å¿«å–"""
        try:
            # æ¸…ç©ºè¨˜æ†¶é«”å¿«å–
            if self.config.enable_memory_cache:
                with self.memory_lock:
                    self.memory_cache.clear()
            
            # æ¸…ç©ºç£ç¢Ÿå¿«å–
            if self.config.enable_disk_cache:
                # åˆªé™¤æ‰€æœ‰å¿«å–æª”æ¡ˆ
                for cache_file in self.cache_dir.rglob("*.cache"):
                    try:
                        cache_file.unlink()
                    except:
                        pass
                
                # æ¸…ç©ºç´¢å¼•
                with sqlite3.connect(str(self.db_path)) as conn:
                    conn.execute('DELETE FROM cache_index')
                    conn.commit()
            
            logger.info("ğŸ§¹ å·²æ¸…ç©ºæ‰€æœ‰å¿«å–")
            
        except Exception as e:
            logger.error(f"æ¸…ç©ºå¿«å–å¤±æ•—: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–å¿«å–çµ±è¨ˆè³‡è¨Š"""
        try:
            # è¨ˆç®—ç¸½å¤§å°
            total_size_bytes = 0
            if self.config.enable_disk_cache:
                with sqlite3.connect(str(self.db_path)) as conn:
                    cursor = conn.execute('SELECT SUM(size_bytes) FROM cache_index')
                    result = cursor.fetchone()
                    if result and result[0]:
                        total_size_bytes = result[0]
            
            # è¨˜æ†¶é«”å¿«å–å¤§å°
            memory_size_bytes = sum(entry.size_bytes for entry in self.memory_cache.values())
            
            total_requests = self.stats['memory_hits'] + self.stats['disk_hits'] + self.stats['misses']
            hit_rate = (
                (self.stats['memory_hits'] + self.stats['disk_hits']) / total_requests * 100
                if total_requests > 0 else 0
            )
            
            return {
                **self.stats,
                'total_size_mb': total_size_bytes / (1024 * 1024),
                'memory_cache_entries': len(self.memory_cache),
                'memory_cache_size_mb': memory_size_bytes / (1024 * 1024),
                'hit_rate': f"{hit_rate:.1f}%",
                'memory_hit_rate': f"{(self.stats['memory_hits'] / total_requests * 100):.1f}%" if total_requests > 0 else "0%",
                'disk_hit_rate': f"{(self.stats['disk_hits'] / total_requests * 100):.1f}%" if total_requests > 0 else "0%",
                'config': asdict(self.config)
            }
            
        except Exception as e:
            logger.error(f"ç²å–å¿«å–çµ±è¨ˆå¤±æ•—: {e}")
            return self.stats
    
    # éåŒæ­¥ä»‹é¢
    async def set_async(self, key: str, value: Any, ttl_hours: Optional[int] = None) -> bool:
        """éåŒæ­¥è¨­ç½®å¿«å–å€¼"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.set, key, value, ttl_hours)
    
    async def get_async(self, key: str) -> Optional[Any]:
        """éåŒæ­¥ç²å–å¿«å–å€¼"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.get, key)
    
    async def delete_async(self, key: str) -> bool:
        """éåŒæ­¥åˆªé™¤å¿«å–å€¼"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.delete, key)