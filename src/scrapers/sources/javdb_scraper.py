# -*- coding: utf-8 -*-
"""
JAVDB å°ˆç”¨çˆ¬èŸ²
é‡å° JAVDB.com å„ªåŒ–çš„çˆ¬èŸ²å¯¦ä½œ
"""

import aiohttp
import logging
import re
from typing import Dict, List, Optional, Any
from urllib.parse import quote, urljoin
from bs4 import BeautifulSoup

from ..base_scraper import BaseScraper, ScrapingException, ErrorType
from ..encoding_utils import create_safe_soup, validate_japanese_content

logger = logging.getLogger(__name__)


class JAVDBScraper(BaseScraper):
    """JAVDB å°ˆç”¨çˆ¬èŸ²é¡"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.base_url = "https://javdb.com"
        self.search_url = f"{self.base_url}/search"
        
        # JAVDB å°ˆç”¨é…ç½®
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8,ja;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': self.base_url,
            'Cache-Control': 'no-cache',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate'
        }
        
        logger.info("ğŸ¬ JAVDB çˆ¬èŸ²å·²åˆå§‹åŒ–")
    
    async def scrape_url(self, url: str) -> Dict[str, Any]:
        """çˆ¬å– JAVDB URL"""
        try:
            timeout = aiohttp.ClientTimeout(total=30)
            
            async with aiohttp.ClientSession(
                headers=self.headers, 
                timeout=timeout
            ) as session:
                async with session.get(url) as response:
                    
                    if response.status == 404:
                        raise ScrapingException(f"é é¢ä¸å­˜åœ¨", ErrorType.CLIENT_ERROR, url, 404)
                    elif response.status >= 500:
                        raise ScrapingException(f"ä¼ºæœå™¨éŒ¯èª¤", ErrorType.SERVER_ERROR, url, response.status)
                    elif response.status == 429:
                        retry_after = response.headers.get('Retry-After')
                        raise ScrapingException(f"è«‹æ±‚éæ–¼é »ç¹", ErrorType.RATE_LIMIT_ERROR, url, 429)
                    
                    response.raise_for_status()
                    
                    # è®€å–å…§å®¹ä¸¦é€²è¡Œç·¨ç¢¼æª¢æ¸¬
                    content_bytes = await response.read()
                    soup, encoding = create_safe_soup(content_bytes)
                    
                    logger.debug(f"âœ… JAVDB é é¢è¼‰å…¥æˆåŠŸï¼Œç·¨ç¢¼: {encoding}")
                    
                    # è§£æå…§å®¹
                    parsed_data = self.parse_content(str(soup), url)
                    parsed_data['source'] = 'JAVDB'
                    parsed_data['encoding'] = encoding
                    
                    return parsed_data
                    
        except aiohttp.ClientError as e:
            raise ScrapingException(f"ç¶²è·¯é€£ç·šéŒ¯èª¤: {e}", ErrorType.NETWORK_ERROR, url)
        except Exception as e:
            if isinstance(e, ScrapingException):
                raise
            raise ScrapingException(f"æœªçŸ¥éŒ¯èª¤: {e}", ErrorType.UNKNOWN_ERROR, url)
    
    def parse_content(self, content: str, url: str) -> Dict[str, Any]:
        """è§£æ JAVDB é é¢å…§å®¹"""
        soup = BeautifulSoup(content, 'html.parser')
        result = {
            'actresses': [],
            'studio': None,
            'studio_code': None,
            'title': None,
            'release_date': None,
            'duration': None,
            'director': None,
            'series': None,
            'rating': None,
            'categories': [],
            'cover_url': None
        }
        
        try:
            # æª¢æŸ¥æ˜¯å¦ç‚ºæœå°‹çµæœé é¢
            if '/search?' in url:
                return self._parse_search_results(soup)
            else:
                return self._parse_detail_page(soup)
                
        except Exception as e:
            logger.error(f"è§£æ JAVDB å…§å®¹å¤±æ•—: {e}")
            raise ScrapingException(f"å…§å®¹è§£æéŒ¯èª¤: {e}", ErrorType.PARSING_ERROR, url)
    
    def _parse_search_results(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """è§£ææœå°‹çµæœé é¢"""
        results = []
        
        # æŸ¥æ‰¾å½±ç‰‡å¡ç‰‡
        movie_items = soup.find_all('div', class_='item')
        
        for item in movie_items:
            try:
                # æå–åŸºæœ¬è³‡è¨Š
                link_element = item.find('a')
                if not link_element:
                    continue
                    
                detail_url = urljoin(self.base_url, link_element.get('href'))
                
                # æå–æ¨™é¡Œ
                title_element = item.find('div', class_='video-title')
                title = title_element.text.strip() if title_element else ''
                
                # æå–æ¼”å“¡è³‡è¨Š
                actresses = []
                actor_elements = item.find_all('a', href=re.compile(r'/actors/'))
                for actor in actor_elements:
                    actress_name = actor.text.strip()
                    if actress_name and self._is_valid_actress_name(actress_name):
                        actresses.append(actress_name)
                
                # æå–ç‰‡å•†è³‡è¨Š
                studio = None
                studio_element = item.find('a', href=re.compile(r'/makers/'))
                if studio_element:
                    studio = studio_element.text.strip()
                
                # æå–ç™¼å¸ƒæ—¥æœŸ
                date_element = item.find('div', class_='meta')
                release_date = None
                if date_element:
                    date_text = date_element.text
                    date_match = re.search(r'(\d{4}-\d{2}-\d{2})', date_text)
                    if date_match:
                        release_date = date_match.group(1)
                
                if actresses or title:
                    results.append({
                        'title': title,
                        'actresses': actresses,
                        'studio': studio,
                        'release_date': release_date,
                        'detail_url': detail_url
                    })
                    
            except Exception as e:
                logger.warning(f"è§£ææœå°‹çµæœé …ç›®å¤±æ•—: {e}")
                continue
        
        return {
            'search_results': results,
            'total_results': len(results)
        }
    
    def _parse_detail_page(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """è§£æè©³æƒ…é é¢"""
        result = {
            'actresses': [],
            'studio': None,
            'studio_code': None,
            'title': None,
            'release_date': None,
            'duration': None,
            'director': None,
            'series': None,
            'rating': None,
            'categories': [],
            'cover_url': None
        }
        
        # æå–æ¨™é¡Œ
        title_element = soup.find('h2', class_='title')
        if title_element:
            result['title'] = title_element.text.strip()
        
        # æå–å°é¢åœ–ç‰‡
        cover_element = soup.find('img', class_='video-cover')
        if cover_element:
            result['cover_url'] = cover_element.get('src')
        
        # æå–è©³ç´°è³‡è¨Š
        info_panels = soup.find_all('div', class_='panel-block')
        
        for panel in info_panels:
            try:
                label_element = panel.find('strong')
                if not label_element:
                    continue
                
                label = label_element.text.strip()
                
                if 'æ¼”å“¡' in label or 'Actor' in label:
                    # æå–æ¼”å“¡
                    actor_links = panel.find_all('a', href=re.compile(r'/actors/'))
                    for link in actor_links:
                        actress_name = link.text.strip()
                        if self._is_valid_actress_name(actress_name):
                            result['actresses'].append(actress_name)
                
                elif 'ç‰‡å•†' in label or 'Maker' in label:
                    # æå–ç‰‡å•†
                    studio_link = panel.find('a', href=re.compile(r'/makers/'))
                    if studio_link:
                        result['studio'] = studio_link.text.strip()
                
                elif 'ç™¼è¡Œæ—¥æœŸ' in label or 'Release Date' in label:
                    # æå–ç™¼è¡Œæ—¥æœŸ
                    date_text = panel.text
                    date_match = re.search(r'(\d{4}-\d{2}-\d{2})', date_text)
                    if date_match:
                        result['release_date'] = date_match.group(1)
                
                elif 'æ™‚é•·' in label or 'Duration' in label:
                    # æå–æ™‚é•·
                    duration_text = panel.text
                    duration_match = re.search(r'(\d+)', duration_text)
                    if duration_match:
                        result['duration'] = f"{duration_match.group(1)}åˆ†é˜"
                
                elif 'å°æ¼”' in label or 'Director' in label:
                    # æå–å°æ¼”
                    director_link = panel.find('a')
                    if director_link:
                        result['director'] = director_link.text.strip()
                
                elif 'ç³»åˆ—' in label or 'Series' in label:
                    # æå–ç³»åˆ—
                    series_link = panel.find('a')
                    if series_link:
                        result['series'] = series_link.text.strip()
                
                elif 'é¡åˆ¥' in label or 'Genre' in label:
                    # æå–é¡åˆ¥
                    category_links = panel.find_all('a', href=re.compile(r'/genres/'))
                    for link in category_links:
                        category = link.text.strip()
                        if category:
                            result['categories'].append(category)
            
            except Exception as e:
                logger.warning(f"è§£æè©³æƒ…é¢æ¿å¤±æ•—: {e}")
                continue
        
        # æå–è©•åˆ†
        rating_element = soup.find('span', class_='score')
        if rating_element:
            try:
                rating_text = rating_element.text.strip()
                rating_match = re.search(r'([\d.]+)', rating_text)
                if rating_match:
                    result['rating'] = float(rating_match.group(1))
            except:
                pass
        
        # å¾æ¨™é¡Œä¸­æå–ç‰‡å•†ä»£ç¢¼
        if result['title']:
            code_match = re.search(r'^([A-Z]+)-?\d+', result['title'])
            if code_match:
                result['studio_code'] = code_match.group(1)
        
        return result
    
    def _is_valid_actress_name(self, name: str) -> bool:
        """é©—è­‰æ˜¯å¦ç‚ºæœ‰æ•ˆçš„å¥³å„ªåç¨±"""
        if not name or len(name) < 2 or len(name) > 30:
            return False
        
        # æ’é™¤å¸¸è¦‹çš„éå¥³å„ªåç¨±
        exclude_keywords = [
            'å‡ºæ¼”è€…', 'æ¼”å“¡', 'å¥³å„ª', 'actor', 'actress', 
            'ä¸æ˜', 'æœªçŸ¥', 'unknown', '---', 'â€“'
        ]
        
        name_lower = name.lower()
        if any(keyword in name_lower for keyword in exclude_keywords):
            return False
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æ—¥æ–‡å­—ç¬¦
        if re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', name):
            return True
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºè¥¿æ–¹åå­—æ ¼å¼
        if re.match(r'^[A-Za-z\s]+$', name) and ' ' in name:
            return True
        
        return False
    
    async def search_video(self, video_code: str) -> Dict[str, Any]:
        """æœå°‹æŒ‡å®šç•ªè™Ÿçš„å½±ç‰‡"""
        search_url = f"{self.search_url}?q={quote(video_code)}&f=all"
        
        try:
            # åŸ·è¡Œå®‰å…¨çˆ¬å–
            result = await self.safe_scrape(search_url)
            
            # å¦‚æœæœ‰æœå°‹çµæœï¼Œå–ç¬¬ä¸€å€‹çµæœçš„è©³æƒ…
            if 'search_results' in result and result['search_results']:
                first_result = result['search_results'][0]
                
                # å¦‚æœæœ‰è©³æƒ…é é¢URLï¼Œé€²ä¸€æ­¥ç²å–è©³ç´°è³‡è¨Š
                if 'detail_url' in first_result:
                    detail_result = await self.safe_scrape(first_result['detail_url'])
                    
                    # åˆä½µæœå°‹çµæœå’Œè©³æƒ…
                    detail_result.update({
                        'video_code': video_code,
                        'search_url': search_url
                    })
                    
                    # é©—è­‰å…§å®¹å“è³ª
                    if detail_result.get('title'):
                        content_quality = validate_japanese_content(detail_result['title'])
                        detail_result['content_quality'] = content_quality
                    
                    return detail_result
                else:
                    # ç›´æ¥è¿”å›æœå°‹çµæœ
                    first_result.update({
                        'video_code': video_code,
                        'search_url': search_url
                    })
                    return first_result
            
            # æ²’æœ‰æ‰¾åˆ°çµæœ
            return {
                'video_code': video_code,
                'search_url': search_url,
                'actresses': [],
                'message': f'æœªæ‰¾åˆ°ç•ªè™Ÿ {video_code} çš„è³‡è¨Š'
            }
            
        except Exception as e:
            logger.error(f"æœå°‹ JAVDB å½±ç‰‡ {video_code} å¤±æ•—: {e}")
            raise ScrapingException(f"æœå°‹å¤±æ•—: {e}", ErrorType.UNKNOWN_ERROR, search_url)
    
    async def get_actress_info(self, actress_name: str) -> Dict[str, Any]:
        """ç²å–å¥³å„ªè³‡è¨Š"""
        search_url = f"{self.search_url}?q={quote(actress_name)}&f=actor"
        
        try:
            result = await self.safe_scrape(search_url)
            
            # æ•´ç†å¥³å„ªä½œå“è³‡è¨Š
            if 'search_results' in result:
                works = result['search_results']
                
                # çµ±è¨ˆç‰‡å•†åˆ†ä½ˆ
                studio_count = {}
                for work in works:
                    studio = work.get('studio')
                    if studio:
                        studio_count[studio] = studio_count.get(studio, 0) + 1
                
                return {
                    'actress_name': actress_name,
                    'total_works': len(works),
                    'works': works,
                    'studio_distribution': studio_count,
                    'search_url': search_url
                }
            
            return {
                'actress_name': actress_name,
                'total_works': 0,
                'works': [],
                'studio_distribution': {},
                'search_url': search_url
            }
            
        except Exception as e:
            logger.error(f"ç²å– JAVDB å¥³å„ªè³‡è¨Š {actress_name} å¤±æ•—: {e}")
            raise ScrapingException(f"ç²å–å¥³å„ªè³‡è¨Šå¤±æ•—: {e}", ErrorType.UNKNOWN_ERROR, search_url)