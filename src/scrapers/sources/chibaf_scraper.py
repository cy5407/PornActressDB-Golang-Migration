# -*- coding: utf-8 -*-
"""
CHIBA-F å°ˆç”¨çˆ¬èŸ²
é‡å° chiba-f.net å„ªåŒ–çš„çˆ¬èŸ²å¯¦ä½œ
"""

import aiohttp
import logging
import re
from typing import Dict, List, Optional, Any
from urllib.parse import quote, urljoin
from bs4 import BeautifulSoup

from ..base_scraper import BaseScraper, ScrapingException, ErrorType
from ..encoding_utils import create_safe_soup, validate_japanese_content

logger = logging.getLogger(__name__)


class ChibaFScraper(BaseScraper):
    """CHIBA-F å°ˆç”¨çˆ¬èŸ²é¡"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.base_url = "https://chiba-f.net"
        self.search_url = f"{self.base_url}/search/"
        
        # CHIBA-F å°ˆç”¨é…ç½®
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en;q=0.8,zh;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': self.base_url,
            'Cache-Control': 'no-cache'
        }
        
        logger.info("ğŸŒ¸ CHIBA-F çˆ¬èŸ²å·²åˆå§‹åŒ–")
    
    async def scrape_url(self, url: str) -> Dict[str, Any]:
        """çˆ¬å– CHIBA-F URL"""
        try:
            timeout = aiohttp.ClientTimeout(total=30)
            
            async with aiohttp.ClientSession(
                headers=self.headers, 
                timeout=timeout
            ) as session:
                async with session.get(url) as response:
                    
                    if response.status == 404:
                        raise ScrapingException(f"é é¢ä¸å­˜åœ¨", ErrorType.CLIENT_ERROR, url, 404)
                    elif response.status >= 500:
                        raise ScrapingException(f"ä¼ºæœå™¨éŒ¯èª¤", ErrorType.SERVER_ERROR, url, response.status)
                    elif response.status == 429:
                        raise ScrapingException(f"è«‹æ±‚éæ–¼é »ç¹", ErrorType.RATE_LIMIT_ERROR, url, 429)
                    
                    response.raise_for_status()
                    
                    # è®€å–å…§å®¹ä¸¦é€²è¡Œç·¨ç¢¼æª¢æ¸¬
                    content_bytes = await response.read()
                    soup, encoding = create_safe_soup(content_bytes)
                    
                    logger.debug(f"âœ… CHIBA-F é é¢è¼‰å…¥æˆåŠŸï¼Œç·¨ç¢¼: {encoding}")
                    
                    # è§£æå…§å®¹
                    parsed_data = self.parse_content(str(soup), url)
                    parsed_data['source'] = 'CHIBA-F'
                    parsed_data['encoding'] = encoding
                    
                    return parsed_data
                    
        except aiohttp.ClientError as e:
            raise ScrapingException(f"ç¶²è·¯é€£ç·šéŒ¯èª¤: {e}", ErrorType.NETWORK_ERROR, url)
        except Exception as e:
            if isinstance(e, ScrapingException):
                raise
            raise ScrapingException(f"æœªçŸ¥éŒ¯èª¤: {e}", ErrorType.UNKNOWN_ERROR, url)
    
    def parse_content(self, content: str, url: str) -> Dict[str, Any]:
        """è§£æ CHIBA-F é é¢å…§å®¹"""
        soup = BeautifulSoup(content, 'html.parser')
        
        try:
            # æª¢æŸ¥æ˜¯å¦ç‚ºæœå°‹çµæœé é¢
            if '/search/' in url:
                return self._parse_search_results(soup)
            else:
                return self._parse_detail_page(soup)
                
        except Exception as e:
            logger.error(f"è§£æ CHIBA-F å…§å®¹å¤±æ•—: {e}")
            raise ScrapingException(f"å…§å®¹è§£æéŒ¯èª¤: {e}", ErrorType.PARSING_ERROR, url)
    
    def _parse_search_results(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """è§£ææœå°‹çµæœé é¢"""
        results = []
        
        # æŸ¥æ‰¾ç”¢å“å€å¡Š
        product_divs = soup.find_all('div', class_='product-div')
        
        for product_div in product_divs:
            try:
                result = self._extract_product_info(product_div)
                if result and result.get('actresses'):
                    results.append(result)
                    
            except Exception as e:
                logger.warning(f"è§£æ CHIBA-F ç”¢å“å€å¡Šå¤±æ•—: {e}")
                continue
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ° product-divï¼Œå˜—è©¦å…¶ä»–çµæ§‹
        if not results:
            # æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„ç”¢å“å®¹å™¨
            containers = (soup.find_all('div', class_='item') or 
                         soup.find_all('div', class_='video') or
                         soup.find_all('article'))
            
            for container in containers:
                try:
                    result = self._extract_generic_info(container)
                    if result and result.get('actresses'):
                        results.append(result)
                        
                except Exception as e:
                    logger.warning(f"è§£æ CHIBA-F é€šç”¨å®¹å™¨å¤±æ•—: {e}")
                    continue
        
        return {
            'search_results': results,
            'total_results': len(results)
        }
    
    def _parse_detail_page(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """è§£æè©³æƒ…é é¢"""
        result = {
            'actresses': [],
            'studio': None,
            'studio_code': None,
            'title': None,
            'release_date': None,
            'series': None,
            'duration': None
        }
        
        # æå–æ¨™é¡Œ
        title_element = (soup.find('h1') or 
                        soup.find('h2') or
                        soup.find('title'))
        if title_element:
            result['title'] = title_element.text.strip()
        
        # æŸ¥æ‰¾è©³ç´°è³‡è¨Šå€å¡Š
        info_sections = soup.find_all(['div', 'section', 'table'])
        
        for section in info_sections:
            try:
                section_text = section.get_text()
                
                # æå–å¥³å„ªåç¨±
                actress_names = self._extract_actresses_from_section(section)
                for name in actress_names:
                    if name not in result['actresses']:
                        result['actresses'].append(name)
                
                # æå–å…¶ä»–è³‡è¨Š
                if 'ç‰‡å•†' in section_text or 'ãƒ¡ãƒ¼ã‚«ãƒ¼' in section_text:
                    studio = self._extract_studio_from_section(section)
                    if studio:
                        result['studio'] = studio
                
                if 'ç™¼è¡Œ' in section_text or 'ç™ºå£²' in section_text:
                    date = self._extract_date_from_section(section)
                    if date:
                        result['release_date'] = date
                
                if 'ç³»åˆ—' in section_text or 'ã‚·ãƒªãƒ¼ã‚º' in section_text:
                    series = self._extract_series_from_section(section)
                    if series:
                        result['series'] = series
                        
            except Exception as e:
                logger.warning(f"è§£æ CHIBA-F è©³æƒ…å€å¡Šå¤±æ•—: {e}")
                continue
        
        # å¾æ¨™é¡Œæå–ç‰‡å•†ä»£ç¢¼
        if result['title']:
            code_match = re.search(r'^([A-Z]+)-?\d+', result['title'])
            if code_match:
                result['studio_code'] = code_match.group(1)
        
        return result
    
    def _extract_product_info(self, product_div) -> Dict[str, Any]:
        """å¾ç”¢å“å€å¡Šæå–è³‡è¨Š"""
        result = {
            'actresses': [],
            'studio': None,
            'studio_code': None,
            'release_date': None,
            'title': None
        }
        
        try:
            # æå–å¥³å„ªåç¨± (é€šå¸¸åœ¨ fw-bold span ä¸­)
            actress_span = product_div.find('span', class_='fw-bold')
            if actress_span:
                actress_name = actress_span.text.strip()
                if self._is_valid_actress_name(actress_name):
                    result['actresses'] = [actress_name]
            
            # æå–ç³»åˆ—/ç‰‡å•†è³‡è¨Š
            series_link = product_div.find('a', href=re.compile(r'../series/'))
            if series_link:
                result['studio'] = series_link.text.strip()
                # å¾ href æå–ç‰‡å•†ä»£ç¢¼
                href = series_link.get('href', '')
                if '../series/' in href:
                    result['studio_code'] = href.replace('../series/', '').strip()
            
            # æå–ç•ªè™Ÿ
            pno_element = product_div.find('div', class_='pno')
            if pno_element:
                pno_text = pno_element.text.strip()
                result['title'] = pno_text
                
                # å¾ç•ªè™Ÿæå–ç‰‡å•†ä»£ç¢¼
                if not result['studio_code']:
                    code_match = re.search(r'^([A-Z]+)', pno_text)
                    if code_match:
                        result['studio_code'] = code_match.group(1)
            
            # æå–ç™¼è¡Œæ—¥æœŸ
            date_span = product_div.find('span', class_='start_date')
            if date_span:
                result['release_date'] = date_span.text.strip()
            
        except Exception as e:
            logger.warning(f"æå– CHIBA-F ç”¢å“è³‡è¨Šå¤±æ•—: {e}")
        
        return result
    
    def _extract_generic_info(self, container) -> Dict[str, Any]:
        """å¾é€šç”¨å®¹å™¨æå–è³‡è¨Š"""
        result = {
            'actresses': [],
            'studio': None,
            'title': None
        }
        
        try:
            container_text = container.get_text()
            
            # æå–å¥³å„ªåç¨±
            actresses = self._extract_actresses_from_text(container_text)
            result['actresses'] = actresses
            
            # æå–æ¨™é¡Œ
            title_element = container.find(['h1', 'h2', 'h3', 'h4'])
            if title_element:
                result['title'] = title_element.text.strip()
            
        except Exception as e:
            logger.warning(f"æå– CHIBA-F é€šç”¨è³‡è¨Šå¤±æ•—: {e}")
        
        return result
    
    def _extract_actresses_from_section(self, section) -> List[str]:
        """å¾å€å¡Šä¸­æå–å¥³å„ªåç¨±"""
        actresses = []
        
        # æŸ¥æ‰¾åŠ ç²—æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯å¥³å„ªåç¨±ï¼‰
        bold_elements = section.find_all(['b', 'strong', 'span'], class_=['fw-bold', 'actress', 'performer'])
        
        for element in bold_elements:
            text = element.text.strip()
            if self._is_valid_actress_name(text):
                actresses.append(text)
        
        # å¾ç´”æ–‡æœ¬ä¸­æå–
        section_text = section.get_text()
        text_actresses = self._extract_actresses_from_text(section_text)
        
        for actress in text_actresses:
            if actress not in actresses:
                actresses.append(actress)
        
        return actresses
    
    def _extract_actresses_from_text(self, text: str) -> List[str]:
        """å¾æ–‡æœ¬ä¸­æå–å¥³å„ªåç¨±"""
        actresses = []
        
        # æ—¥æ–‡å§“åæ¨¡å¼
        name_patterns = [
            r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]{2,8}\s*[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]{2,8}',  # å§“åçµ„åˆ
            r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]{3,15}'  # å–®ä¸€åç¨±
        ]
        
        for pattern in name_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                clean_name = match.strip()
                if self._is_valid_actress_name(clean_name) and clean_name not in actresses:
                    actresses.append(clean_name)
        
        return actresses
    
    def _extract_studio_from_section(self, section) -> Optional[str]:
        """å¾å€å¡Šä¸­æå–ç‰‡å•†åç¨±"""
        # æŸ¥æ‰¾ç‰‡å•†é€£çµ
        studio_link = section.find('a', href=re.compile(r'maker|studio|series'))
        if studio_link:
            return studio_link.text.strip()
        
        # å¾æ–‡æœ¬ä¸­æå–
        section_text = section.get_text()
        studio_match = re.search(r'(?:ç‰‡å•†|ãƒ¡ãƒ¼ã‚«ãƒ¼)[ï¼š:]\s*([^\n\r]+)', section_text)
        if studio_match:
            return studio_match.group(1).strip()
        
        return None
    
    def _extract_date_from_section(self, section) -> Optional[str]:
        """å¾å€å¡Šä¸­æå–æ—¥æœŸ"""
        section_text = section.get_text()
        date_patterns = [
            r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
            r'(\d{4}\.\d{1,2}\.\d{1,2})',
            r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)'
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, section_text)
            if match:
                return match.group(1)
        
        return None
    
    def _extract_series_from_section(self, section) -> Optional[str]:
        """å¾å€å¡Šä¸­æå–ç³»åˆ—åç¨±"""
        # æŸ¥æ‰¾ç³»åˆ—é€£çµ
        series_link = section.find('a', href=re.compile(r'series'))
        if series_link:
            return series_link.text.strip()
        
        # å¾æ–‡æœ¬ä¸­æå–
        section_text = section.get_text()
        series_match = re.search(r'(?:ç³»åˆ—|ã‚·ãƒªãƒ¼ã‚º)[ï¼š:]\s*([^\n\r]+)', section_text)
        if series_match:
            return series_match.group(1).strip()
        
        return None
    
    def _is_valid_actress_name(self, name: str) -> bool:
        """é©—è­‰æ˜¯å¦ç‚ºæœ‰æ•ˆçš„å¥³å„ªåç¨±"""
        if not name or len(name) < 2 or len(name) > 25:
            return False
        
        # æ’é™¤å¸¸è¦‹çš„éå¥³å„ªé—œéµè©
        exclude_keywords = [
            'ã‚µãƒ³ãƒ—ãƒ«', 'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰', 'ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼', 'ãƒ¬ãƒ“ãƒ¥ãƒ¼', 'å‹•ç”»',
            'æ¤œç´¢', 'ä»¶', 'çµæœ', 'ãƒšãƒ¼ã‚¸', 'ãƒ¡ãƒ‹ãƒ¥ãƒ¼', 'ãƒ­ã‚°ã‚¤ãƒ³',
            'sample', 'download', 'preview', 'review', 'video',
            'search', 'result', 'page', 'menu', 'login'
        ]
        
        name_lower = name.lower()
        if any(keyword in name_lower for keyword in exclude_keywords):
            return False
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æ•¸å­—éå¤š
        if len(re.findall(r'\d', name)) > len(name) // 3:
            return False
        
        # å¿…é ˆåŒ…å«æ—¥æ–‡å­—ç¬¦
        if re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', name):
            return True
        
        return False
    
    async def search_video(self, video_code: str) -> Dict[str, Any]:
        """æœå°‹æŒ‡å®šç•ªè™Ÿçš„å½±ç‰‡"""
        search_url = f"{self.search_url}?keyword={quote(video_code)}"
        
        try:
            # åŸ·è¡Œå®‰å…¨çˆ¬å–
            result = await self.safe_scrape(search_url)
            
            # æŸ¥æ‰¾åŒ¹é…çš„çµæœ
            if 'search_results' in result and result['search_results']:
                # å°‹æ‰¾æœ€åŒ¹é…çš„çµæœ
                best_match = None
                for item in result['search_results']:
                    title = item.get('title', '')
                    if video_code.upper() in title.upper():
                        best_match = item
                        break
                
                if best_match:
                    # é©—è­‰å…§å®¹å“è³ª
                    content_quality = {}
                    for actress in best_match.get('actresses', []):
                        quality = validate_japanese_content(actress)
                        content_quality[actress] = quality
                    
                    best_match.update({
                        'video_code': video_code,
                        'search_url': search_url,
                        'content_quality': content_quality
                    })
                    
                    return best_match
                else:
                    # è¿”å›ç¬¬ä¸€å€‹çµæœ
                    first_result = result['search_results'][0]
                    first_result.update({
                        'video_code': video_code,
                        'search_url': search_url
                    })
                    return first_result
            
            # æ²’æœ‰æ‰¾åˆ°çµæœ
            return {
                'video_code': video_code,
                'search_url': search_url,
                'actresses': [],
                'message': f'æœªæ‰¾åˆ°ç•ªè™Ÿ {video_code} çš„è³‡è¨Š'
            }
            
        except Exception as e:
            logger.error(f"æœå°‹ CHIBA-F å½±ç‰‡ {video_code} å¤±æ•—: {e}")
            raise ScrapingException(f"æœå°‹å¤±æ•—: {e}", ErrorType.UNKNOWN_ERROR, search_url)
    
    async def get_actress_info(self, actress_name: str) -> Dict[str, Any]:
        """ç²å–å¥³å„ªè³‡è¨Š"""
        search_url = f"{self.search_url}?keyword={quote(actress_name)}"
        
        try:
            result = await self.safe_scrape(search_url)
            
            # è™•ç†å¥³å„ªä½œå“
            works = []
            if 'search_results' in result:
                # ç¯©é¸åŒ…å«è©²å¥³å„ªçš„ä½œå“
                for item in result['search_results']:
                    actresses = item.get('actresses', [])
                    if any(actress_name in actress for actress in actresses):
                        works.append(item)
            
            return {
                'actress_name': actress_name,
                'total_works': len(works),
                'works': works,
                'search_url': search_url
            }
            
        except Exception as e:
            logger.error(f"ç²å– CHIBA-F å¥³å„ªè³‡è¨Š {actress_name} å¤±æ•—: {e}")
            raise ScrapingException(f"ç²å–å¥³å„ªè³‡è¨Šå¤±æ•—: {e}", ErrorType.UNKNOWN_ERROR, search_url)