# -*- coding: utf-8 -*-
"""
AV-WIKI å°ˆç”¨çˆ¬èŸ²
é‡å° av-wiki.net å„ªåŒ–çš„çˆ¬èŸ²å¯¦ä½œ
"""

import aiohttp
import logging
import re
from typing import Dict, List, Optional, Any
from urllib.parse import quote, urljoin
from bs4 import BeautifulSoup

from ..base_scraper import BaseScraper, ScrapingException, ErrorType
from ..encoding_utils import create_safe_soup, validate_japanese_content

logger = logging.getLogger(__name__)


class AVWikiScraper(BaseScraper):
    """AV-WIKI å°ˆç”¨çˆ¬èŸ²é¡"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.base_url = "https://av-wiki.net"
        
        # AV-WIKI å°ˆç”¨é…ç½®
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en;q=0.8,zh;q=0.7',  # æ—¥èªå„ªå…ˆ
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': self.base_url,
            'Cache-Control': 'no-cache'
        }
        
        logger.info("ğŸ“š AV-WIKI çˆ¬èŸ²å·²åˆå§‹åŒ–")
    
    async def scrape_url(self, url: str) -> Dict[str, Any]:
        """çˆ¬å– AV-WIKI URL"""
        try:
            timeout = aiohttp.ClientTimeout(total=30)
            
            async with aiohttp.ClientSession(
                headers=self.headers, 
                timeout=timeout
            ) as session:
                async with session.get(url) as response:
                    
                    if response.status == 404:
                        raise ScrapingException(f"é é¢ä¸å­˜åœ¨", ErrorType.CLIENT_ERROR, url, 404)
                    elif response.status >= 500:
                        raise ScrapingException(f"ä¼ºæœå™¨éŒ¯èª¤", ErrorType.SERVER_ERROR, url, response.status)
                    elif response.status == 429:
                        raise ScrapingException(f"è«‹æ±‚éæ–¼é »ç¹", ErrorType.RATE_LIMIT_ERROR, url, 429)
                    
                    response.raise_for_status()
                    
                    # è®€å–å…§å®¹ä¸¦é€²è¡Œç·¨ç¢¼æª¢æ¸¬
                    content_bytes = await response.read()
                    soup, encoding = create_safe_soup(content_bytes)
                    
                    logger.debug(f"âœ… AV-WIKI é é¢è¼‰å…¥æˆåŠŸï¼Œç·¨ç¢¼: {encoding}")
                    
                    # è§£æå…§å®¹
                    parsed_data = self.parse_content(str(soup), url)
                    parsed_data['source'] = 'AV-WIKI'
                    parsed_data['encoding'] = encoding
                    
                    return parsed_data
                    
        except aiohttp.ClientError as e:
            raise ScrapingException(f"ç¶²è·¯é€£ç·šéŒ¯èª¤: {e}", ErrorType.NETWORK_ERROR, url)
        except Exception as e:
            if isinstance(e, ScrapingException):
                raise
            raise ScrapingException(f"æœªçŸ¥éŒ¯èª¤: {e}", ErrorType.UNKNOWN_ERROR, url)
    
    def parse_content(self, content: str, url: str) -> Dict[str, Any]:
        """è§£æ AV-WIKI é é¢å…§å®¹"""
        soup = BeautifulSoup(content, 'html.parser')
        result = {
            'actresses': [],
            'studio': None,
            'studio_code': None,
            'title': None,
            'release_date': None,
            'series': None,
            'categories': []
        }
        
        try:
            # æª¢æŸ¥æ˜¯å¦ç‚ºæœå°‹çµæœé é¢
            if '?s=' in url:
                return self._parse_search_results(soup)
            else:
                return self._parse_detail_page(soup)
                
        except Exception as e:
            logger.error(f"è§£æ AV-WIKI å…§å®¹å¤±æ•—: {e}")
            raise ScrapingException(f"å…§å®¹è§£æéŒ¯èª¤: {e}", ErrorType.PARSING_ERROR, url)
    
    def _parse_search_results(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """è§£ææœå°‹çµæœé é¢"""
        results = []
        
        # æ–¹æ³•1: å°‹æ‰¾å°ˆç”¨çš„å¥³å„ªåç¨±å…ƒç´ 
        actress_elements = soup.find_all(class_="actress-name")
        if actress_elements:
            for element in actress_elements:
                actress_name = element.text.strip()
                if self._is_valid_actress_name(actress_name):
                    results.append({
                        'actresses': [actress_name],
                        'source_element': 'actress-name'
                    })
        
        # æ–¹æ³•2: æœå°‹ç”¢å“æ–‡ç« 
        articles = soup.find_all('article') or soup.find_all('div', class_='post')
        
        for article in articles:
            try:
                # æå–æ¨™é¡Œ
                title_element = (article.find('h1') or 
                               article.find('h2') or 
                               article.find('h3') or
                               article.find(class_='entry-title'))
                
                title = title_element.text.strip() if title_element else ''
                
                # å¾å…§å®¹ä¸­æå–å¥³å„ªåç¨±
                actresses = self._extract_actresses_from_text(article.get_text())
                
                # æå–é€£çµï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
                link_element = article.find('a')
                detail_url = link_element.get('href') if link_element else None
                if detail_url and not detail_url.startswith('http'):
                    detail_url = urljoin(self.base_url, detail_url)
                
                if actresses:
                    results.append({
                        'title': title,
                        'actresses': actresses,
                        'detail_url': detail_url,
                        'source_element': 'article'
                    })
                    
            except Exception as e:
                logger.warning(f"è§£æ AV-WIKI æ–‡ç« å¤±æ•—: {e}")
                continue
        
        # æ–¹æ³•3: é€šç”¨æ–‡æœ¬æƒæ
        if not results:
            page_text = soup.get_text()
            lines = [line.strip() for line in page_text.split('\n') if line.strip()]
            
            for i, line in enumerate(lines):
                if any(keyword in line for keyword in ['å‡ºæ¼”', 'å¥³å„ª', 'æ¼”å“¡']):
                    # æª¢æŸ¥å‰å¾Œå¹¾è¡Œæ˜¯å¦æœ‰å¥³å„ªåç¨±
                    for j in range(max(0, i-2), min(len(lines), i+3)):
                        potential_names = self._extract_actresses_from_text(lines[j])
                        if potential_names:
                            results.append({
                                'actresses': potential_names,
                                'source_element': 'text_scan',
                                'context_line': line
                            })
        
        # å»é‡ä¸¦è¿”å›
        unique_actresses = set()
        filtered_results = []
        
        for result in results:
            for actress in result['actresses']:
                if actress not in unique_actresses:
                    unique_actresses.add(actress)
                    filtered_results.append({
                        'actresses': [actress],
                        'source': result.get('source_element', 'unknown')
                    })
        
        return {
            'search_results': filtered_results,
            'total_results': len(filtered_results),
            'unique_actresses': list(unique_actresses)
        }
    
    def _parse_detail_page(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """è§£æè©³æƒ…é é¢"""
        result = {
            'actresses': [],
            'studio': None,
            'studio_code': None,
            'title': None,
            'release_date': None,
            'series': None,
            'categories': []
        }
        
        # æå–æ¨™é¡Œ
        title_element = (soup.find('h1') or 
                        soup.find('h2', class_='entry-title') or
                        soup.find('title'))
        if title_element:
            result['title'] = title_element.text.strip()
        
        # å¾é é¢å…§å®¹ä¸­æå–è³‡è¨Š
        page_text = soup.get_text()
        
        # æå–å¥³å„ªåç¨±
        result['actresses'] = self._extract_actresses_from_text(page_text)
        
        # æå–ç‰‡å•†è³‡è¨Š
        studio_info = self._extract_studio_info(page_text, result.get('title', ''))
        result.update(studio_info)
        
        # æå–ç™¼è¡Œæ—¥æœŸ
        date_match = re.search(r'(\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2})', page_text)
        if date_match:
            result['release_date'] = date_match.group(1)
        
        return result
    
    def _extract_actresses_from_text(self, text: str) -> List[str]:
        """å¾æ–‡æœ¬ä¸­æå–å¥³å„ªåç¨±"""
        actresses = []
        
        # åˆ†å‰²æˆè¡Œ
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        
        for line in lines:
            # å°‹æ‰¾å¯èƒ½çš„å¥³å„ªåç¨±
            potential_names = re.findall(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]{2,20}', line)
            
            for name in potential_names:
                if self._is_valid_actress_name(name) and name not in actresses:
                    actresses.append(name)
        
        return actresses
    
    def _extract_studio_info(self, text: str, title: str = '') -> Dict[str, Any]:
        """æå–ç‰‡å•†è³‡è¨Š"""
        studio_info = {
            'studio': None,
            'studio_code': None
        }
        
        # å¾æ¨™é¡Œä¸­æå–ç‰‡å•†ä»£ç¢¼
        if title:
            code_match = re.search(r'^([A-Z]+)-?\d+', title)
            if code_match:
                studio_info['studio_code'] = code_match.group(1)
        
        # å¾æ–‡æœ¬ä¸­æå–ç‰‡å•†åç¨±
        studio_patterns = [
            r'ç‰‡å•†[ï¼š:]\s*([^\n\r]+)',
            r'è£½ä½œ[ï¼š:]\s*([^\n\r]+)', 
            r'ãƒ¡ãƒ¼ã‚«ãƒ¼[ï¼š:]\s*([^\n\r]+)',
            r'(S1|SOD|MOODYZ|PREMIUM|WANZ|FALENO|ATTACKERS|E-BODY|KAWAII|FITCH|MADONNA)',
        ]
        
        for pattern in studio_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                studio_name = match.group(1).strip()
                if len(studio_name) < 50:  # åˆç†é•·åº¦é™åˆ¶
                    studio_info['studio'] = studio_name
                    break
        
        return studio_info
    
    def _is_valid_actress_name(self, name: str) -> bool:
        """é©—è­‰æ˜¯å¦ç‚ºæœ‰æ•ˆçš„å¥³å„ªåç¨±"""
        if not name or len(name) < 2 or len(name) > 20:
            return False
        
        # æ’é™¤å¸¸è¦‹çš„éå¥³å„ªé—œéµè©
        exclude_keywords = [
            'SOD', 'STARS', 'FANZA', 'MGS', 'MIDV', 'SSIS', 'IPX', 'IPZZ',
            'ç¶šãã‚’èª­ã‚€', 'æ¤œç´¢', 'ä»¶', 'ç‰¹å…¸', 'æ˜ åƒ', 'ä»˜ã', 'star', 'SOKMIL',
            'Menu', 'ã‚»ãƒ¼ãƒ«', 'é™å®š', 'æœ€å¤§', 'å‡ºæ¼”è€…', 'å¥³å„ª', 'æ¼”å“¡', 'actress',
            'å‹•ç”»', 'ã‚µãƒ³ãƒ—ãƒ«', 'ãƒ¬ãƒ“ãƒ¥ãƒ¼', 'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰'
        ]
        
        if any(keyword in name for keyword in exclude_keywords):
            return False
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æ•¸å­—éå¤šï¼ˆå¯èƒ½æ˜¯ç·¨è™Ÿï¼‰
        if re.match(r'^\d+$', name) or len(re.findall(r'\d', name)) > len(name) // 2:
            return False
        
        # å¿…é ˆåŒ…å«æ—¥æ–‡å­—ç¬¦
        if re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', name):
            return True
        
        return False
    
    async def search_video(self, video_code: str) -> Dict[str, Any]:
        """æœå°‹æŒ‡å®šç•ªè™Ÿçš„å½±ç‰‡"""
        search_url = f"{self.base_url}/?s={quote(video_code)}&post_type=product"
        
        try:
            # åŸ·è¡Œå®‰å…¨çˆ¬å–
            result = await self.safe_scrape(search_url)
            
            # è™•ç†æœå°‹çµæœ
            if 'unique_actresses' in result and result['unique_actresses']:
                actresses = result['unique_actresses']
                
                # é©—è­‰å…§å®¹å“è³ª
                content_quality = {}
                for actress in actresses:
                    quality = validate_japanese_content(actress)
                    content_quality[actress] = quality
                
                return {
                    'video_code': video_code,
                    'search_url': search_url,
                    'actresses': actresses,
                    'content_quality': content_quality,
                    'search_results': result.get('search_results', [])
                }
            
            # æ²’æœ‰æ‰¾åˆ°çµæœ
            return {
                'video_code': video_code,
                'search_url': search_url,
                'actresses': [],
                'message': f'æœªæ‰¾åˆ°ç•ªè™Ÿ {video_code} çš„è³‡è¨Š'
            }
            
        except Exception as e:
            logger.error(f"æœå°‹ AV-WIKI å½±ç‰‡ {video_code} å¤±æ•—: {e}")
            raise ScrapingException(f"æœå°‹å¤±æ•—: {e}", ErrorType.UNKNOWN_ERROR, search_url)
    
    async def get_actress_info(self, actress_name: str) -> Dict[str, Any]:
        """ç²å–å¥³å„ªè³‡è¨Š"""
        search_url = f"{self.base_url}/?s={quote(actress_name)}"
        
        try:
            result = await self.safe_scrape(search_url)
            
            # è™•ç†å¥³å„ªè³‡è¨Š
            works = []
            if 'search_results' in result:
                works = result['search_results']
            
            return {
                'actress_name': actress_name,
                'total_works': len(works),
                'works': works,
                'search_url': search_url
            }
            
        except Exception as e:
            logger.error(f"ç²å– AV-WIKI å¥³å„ªè³‡è¨Š {actress_name} å¤±æ•—: {e}")
            raise ScrapingException(f"ç²å–å¥³å„ªè³‡è¨Šå¤±æ•—: {e}", ErrorType.UNKNOWN_ERROR, search_url)