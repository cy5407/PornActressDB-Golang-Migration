# -*- coding: utf-8 -*-
"""
çµ±ä¸€çˆ¬èŸ²ç®¡ç†å™¨
æ•´åˆæ‰€æœ‰è³‡æ–™æºçš„çµ±ä¸€ä»‹é¢
"""

import asyncio
import logging
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass
from enum import Enum

from .sources import JAVDBScraper, AVWikiScraper, ChibaFScraper
from .cache_manager import CacheManager, CacheConfig
from .rate_limiter import RateLimiter, DomainConfig
from .encoding_utils import install_encoding_warning_filter
from .base_scraper import RetryConfig, HealthCheckConfig

logger = logging.getLogger(__name__)

# å®‰è£ç·¨ç¢¼è­¦å‘Šéæ¿¾å™¨
install_encoding_warning_filter()


class DataSource(Enum):
    """è³‡æ–™æºæšèˆ‰"""
    JAVDB = "javdb"
    AVWIKI = "avwiki" 
    CHIBAF = "chibaf"


@dataclass
class UnifiedScraperConfig:
    """çµ±ä¸€çˆ¬èŸ²é…ç½®"""
    # è³‡æ–™æºå„ªå…ˆç´š
    source_priority: List[DataSource] = None
    
    # ä½µç™¼è¨­å®š
    max_concurrent_sources: int = 2
    source_timeout: float = 30.0
    
    # é‡è©¦è¨­å®š
    retry_config: RetryConfig = None
    
    # å¿«å–è¨­å®š
    cache_config: CacheConfig = None
    
    # å¥åº·æª¢æŸ¥è¨­å®š
    health_config: HealthCheckConfig = None
    
    # çµæœåˆä½µè¨­å®š
    merge_results: bool = True
    require_consensus: bool = False  # æ˜¯å¦éœ€è¦å¤šå€‹æºçš„å…±è­˜
    min_consensus_sources: int = 2
    
    def __post_init__(self):
        if self.source_priority is None:
            self.source_priority = [DataSource.JAVDB, DataSource.AVWIKI, DataSource.CHIBAF]
        
        if self.retry_config is None:
            self.retry_config = RetryConfig()
        
        if self.cache_config is None:
            self.cache_config = CacheConfig()
        
        if self.health_config is None:
            self.health_config = HealthCheckConfig()


class UnifiedWebScraper:
    """çµ±ä¸€ç¶²è·¯çˆ¬èŸ²ç®¡ç†å™¨"""
    
    def __init__(self, config: UnifiedScraperConfig = None):
        self.config = config or UnifiedScraperConfig()
        
        # åˆå§‹åŒ–å¿«å–å’Œé™æµå™¨
        self.cache_manager = CacheManager(self.config.cache_config)
        self.rate_limiter = RateLimiter()
        
        # é…ç½®å„åŸŸåçš„é™æµè¦å‰‡
        self._configure_domain_limits()
        
        # åˆå§‹åŒ–å„è³‡æ–™æºçˆ¬èŸ²
        self.scrapers = {
            DataSource.JAVDB: JAVDBScraper(
                cache_manager=self.cache_manager,
                rate_limiter=self.rate_limiter
            ),
            DataSource.AVWIKI: AVWikiScraper(
                cache_manager=self.cache_manager,
                rate_limiter=self.rate_limiter
            ),
            DataSource.CHIBAF: ChibaFScraper(
                cache_manager=self.cache_manager,
                rate_limiter=self.rate_limiter
            )
        }
        
        # çµ±è¨ˆè³‡è¨Š
        self.stats = {
            'total_searches': 0,
            'successful_searches': 0,
            'failed_searches': 0,
            'source_usage': {source: 0 for source in DataSource},
            'source_success_rate': {source: 0.0 for source in DataSource},
            'cache_hits': 0,
            'merged_results': 0
        }
        
        logger.info("ğŸš€ çµ±ä¸€çˆ¬èŸ²ç®¡ç†å™¨å·²åˆå§‹åŒ–")
    
    def _configure_domain_limits(self):
        """é…ç½®å„åŸŸåçš„é™æµè¦å‰‡"""
        domain_configs = {
            'javdb.com': DomainConfig(
                requests_per_minute=20,
                requests_per_hour=800,
                burst_limit=5,
                min_interval=1.0,
                max_interval=3.0
            ),
            'av-wiki.net': DomainConfig(
                requests_per_minute=10,
                requests_per_hour=300,
                burst_limit=3,
                min_interval=2.0,
                max_interval=5.0
            ),
            'chiba-f.net': DomainConfig(
                requests_per_minute=15,
                requests_per_hour=500,
                burst_limit=4,
                min_interval=1.5,
                max_interval=4.0
            )
        }
        
        for domain, config in domain_configs.items():
            self.rate_limiter.add_domain_config(domain, config)
    
    async def search_video_info(self, video_code: str, sources: List[DataSource] = None) -> Dict[str, Any]:
        """
        å¾å¤šå€‹è³‡æ–™æºæœå°‹å½±ç‰‡è³‡è¨Š
        
        Args:
            video_code: å½±ç‰‡ç•ªè™Ÿ
            sources: æŒ‡å®šä½¿ç”¨çš„è³‡æ–™æºåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰é…ç½®çš„æº
            
        Returns:
            åˆä½µå¾Œçš„å½±ç‰‡è³‡è¨Š
        """
        self.stats['total_searches'] += 1
        
        if sources is None:
            sources = self.config.source_priority
        
        # æª¢æŸ¥å¿«å–
        cache_key = f"video:{video_code}"
        cached_result = await self.cache_manager.get_async(cache_key)
        if cached_result:
            self.stats['cache_hits'] += 1
            logger.info(f"ğŸ“‹ å¾å¿«å–ç²å–å½±ç‰‡è³‡è¨Š: {video_code}")
            return cached_result
        
        # ä¸¦è¡Œæœå°‹
        logger.info(f"ğŸ” é–‹å§‹æœå°‹å½±ç‰‡ {video_code}ï¼Œä½¿ç”¨è³‡æ–™æº: {[s.value for s in sources]}")
        
        results = await self._search_from_multiple_sources(
            video_code, 
            sources, 
            'search_video'
        )
        
        # åˆä½µçµæœ
        merged_result = self._merge_video_results(results, video_code)
        
        # å„²å­˜åˆ°å¿«å–
        if merged_result.get('actresses'):
            await self.cache_manager.set_async(cache_key, merged_result, ttl_hours=24)
        
        # æ›´æ–°çµ±è¨ˆ
        if merged_result.get('actresses'):
            self.stats['successful_searches'] += 1
        else:
            self.stats['failed_searches'] += 1
        
        logger.info(f"âœ… å½±ç‰‡æœå°‹å®Œæˆ: {video_code}")
        return merged_result
    
    async def get_actress_info(self, actress_name: str, sources: List[DataSource] = None) -> Dict[str, Any]:
        """
        å¾å¤šå€‹è³‡æ–™æºç²å–å¥³å„ªè³‡è¨Š
        
        Args:
            actress_name: å¥³å„ªåç¨±
            sources: æŒ‡å®šä½¿ç”¨çš„è³‡æ–™æºåˆ—è¡¨
            
        Returns:
            åˆä½µå¾Œçš„å¥³å„ªè³‡è¨Š
        """
        if sources is None:
            sources = self.config.source_priority
        
        # æª¢æŸ¥å¿«å–
        cache_key = f"actress:{actress_name}"
        cached_result = await self.cache_manager.get_async(cache_key)
        if cached_result:
            self.stats['cache_hits'] += 1
            logger.info(f"ğŸ“‹ å¾å¿«å–ç²å–å¥³å„ªè³‡è¨Š: {actress_name}")
            return cached_result
        
        logger.info(f"ğŸ‘¤ é–‹å§‹æœå°‹å¥³å„ª {actress_name}ï¼Œä½¿ç”¨è³‡æ–™æº: {[s.value for s in sources]}")
        
        results = await self._search_from_multiple_sources(
            actress_name,
            sources,
            'get_actress_info'
        )
        
        # åˆä½µçµæœ
        merged_result = self._merge_actress_results(results, actress_name)
        
        # å„²å­˜åˆ°å¿«å–
        if merged_result.get('total_works', 0) > 0:
            await self.cache_manager.set_async(cache_key, merged_result, ttl_hours=12)
        
        logger.info(f"âœ… å¥³å„ªæœå°‹å®Œæˆ: {actress_name}")
        return merged_result
    
    async def _search_from_multiple_sources(
        self, 
        query: str, 
        sources: List[DataSource], 
        method_name: str
    ) -> Dict[DataSource, Any]:
        """å¾å¤šå€‹è³‡æ–™æºä½µç™¼æœå°‹"""
        
        # é™åˆ¶ä½µç™¼æ•¸
        semaphore = asyncio.Semaphore(self.config.max_concurrent_sources)
        
        async def search_single_source(source: DataSource) -> tuple:
            async with semaphore:
                try:
                    scraper = self.scrapers[source]
                    method = getattr(scraper, method_name)
                    
                    # è¨­ç½®è¶…æ™‚
                    result = await asyncio.wait_for(
                        method(query),
                        timeout=self.config.source_timeout
                    )
                    
                    self.stats['source_usage'][source] += 1
                    logger.debug(f"âœ… {source.value} æœå°‹æˆåŠŸ: {query}")
                    return source, result
                    
                except asyncio.TimeoutError:
                    logger.warning(f"â° {source.value} æœå°‹è¶…æ™‚: {query}")
                    return source, None
                except Exception as e:
                    logger.error(f"âŒ {source.value} æœå°‹å¤±æ•—: {query} - {e}")
                    return source, None
        
        # ä½µç™¼åŸ·è¡Œæ‰€æœ‰æœå°‹
        tasks = [search_single_source(source) for source in sources]
        completed_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ•´ç†çµæœ
        results = {}
        for item in completed_results:
            if isinstance(item, tuple):
                source, result = item
                if result is not None:
                    results[source] = result
        
        return results
    
    def _merge_video_results(self, results: Dict[DataSource, Any], video_code: str) -> Dict[str, Any]:
        """åˆä½µå½±ç‰‡æœå°‹çµæœ"""
        if not results:
            return {
                'video_code': video_code,
                'actresses': [],
                'source': 'none',
                'message': 'æ‰€æœ‰è³‡æ–™æºéƒ½æœªæ‰¾åˆ°çµæœ'
            }
        
        # å¦‚æœåªæœ‰ä¸€å€‹çµæœï¼Œç›´æ¥è¿”å›
        if len(results) == 1:
            source, result = next(iter(results.items()))
            result['primary_source'] = source.value
            return result
        
        # åˆä½µå¤šå€‹çµæœ
        merged = {
            'video_code': video_code,
            'actresses': [],
            'studio': None,
            'studio_code': None,
            'title': None,
            'release_date': None,
            'duration': None,
            'director': None,
            'series': None,
            'rating': None,
            'categories': [],
            'sources_used': list(results.keys()),
            'source_count': len(results)
        }
        
        # æ”¶é›†æ‰€æœ‰å¥³å„ªåç¨±
        all_actresses = set()
        actress_votes = {}
        
        for source, result in results.items():
            actresses = result.get('actresses', [])
            for actress in actresses:
                all_actresses.add(actress)
                actress_votes[actress] = actress_votes.get(actress, 0) + 1
        
        # å¦‚æœéœ€è¦å…±è­˜ï¼Œåªä¿ç•™å¤šå€‹æºéƒ½ç¢ºèªçš„å¥³å„ª
        if self.config.require_consensus and len(results) >= self.config.min_consensus_sources:
            min_votes = self.config.min_consensus_sources
            merged['actresses'] = [
                actress for actress, votes in actress_votes.items() 
                if votes >= min_votes
            ]
            merged['consensus_required'] = True
        else:
            merged['actresses'] = list(all_actresses)
            merged['consensus_required'] = False
        
        # åˆä½µå…¶ä»–è³‡è¨Šï¼ˆå„ªå…ˆç´šé †åºï¼‰
        for source in self.config.source_priority:
            if source in results:
                result = results[source]
                
                # ä½¿ç”¨ç¬¬ä¸€å€‹å¯ç”¨çš„å€¼
                for field in ['studio', 'studio_code', 'title', 'release_date', 
                             'duration', 'director', 'series', 'rating']:
                    if not merged[field] and result.get(field):
                        merged[field] = result[field]
                        merged[f'{field}_source'] = source.value
                
                # åˆä½µé¡åˆ¥
                categories = result.get('categories', [])
                for category in categories:
                    if category not in merged['categories']:
                        merged['categories'].append(category)
        
        # è¨­ç½®ä¸»è¦è³‡æ–™æº
        merged['primary_source'] = self.config.source_priority[0].value
        
        if self.config.merge_results:
            self.stats['merged_results'] += 1
        
        return merged
    
    def _merge_actress_results(self, results: Dict[DataSource, Any], actress_name: str) -> Dict[str, Any]:
        """åˆä½µå¥³å„ªè³‡è¨Šçµæœ"""
        if not results:
            return {
                'actress_name': actress_name,
                'total_works': 0,
                'works': [],
                'studio_distribution': {},
                'sources_used': []
            }
        
        # åˆä½µæ‰€æœ‰ä½œå“
        all_works = []
        studio_count = {}
        
        for source, result in results.items():
            works = result.get('works', [])
            all_works.extend(works)
            
            # åˆä½µç‰‡å•†çµ±è¨ˆ
            source_studios = result.get('studio_distribution', {})
            for studio, count in source_studios.items():
                studio_count[studio] = studio_count.get(studio, 0) + count
        
        # å»é‡ä½œå“ï¼ˆåŸºæ–¼æ¨™é¡Œï¼‰
        unique_works = []
        seen_titles = set()
        
        for work in all_works:
            title = work.get('title', '')
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_works.append(work)
        
        return {
            'actress_name': actress_name,
            'total_works': len(unique_works),
            'works': unique_works,
            'studio_distribution': studio_count,
            'sources_used': [source.value for source in results.keys()],
            'source_count': len(results)
        }
    
    async def batch_search_videos(
        self, 
        video_codes: List[str], 
        progress_callback: Optional[Callable[[str], None]] = None
    ) -> Dict[str, Any]:
        """æ‰¹æ¬¡æœå°‹å¤šå€‹å½±ç‰‡"""
        
        if not video_codes:
            return {}
        
        logger.info(f"ğŸ“¦ é–‹å§‹æ‰¹æ¬¡æœå°‹ {len(video_codes)} å€‹å½±ç‰‡")
        
        # åˆ†æ‰¹è™•ç†ä»¥æ§åˆ¶ä½µç™¼
        batch_size = 10
        all_results = {}
        
        for i in range(0, len(video_codes), batch_size):
            batch_codes = video_codes[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            total_batches = (len(video_codes) + batch_size - 1) // batch_size
            
            if progress_callback:
                progress_callback(f"è™•ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_codes)} å€‹å½±ç‰‡)...")
            
            logger.info(f"ğŸ“¦ è™•ç†æ‰¹æ¬¡ {batch_num}/{total_batches}")
            
            # ä¸¦è¡Œè™•ç†ç•¶å‰æ‰¹æ¬¡
            tasks = [self.search_video_info(code) for code in batch_codes]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # æ•´ç†æ‰¹æ¬¡çµæœ
            for j, result in enumerate(batch_results):
                code = batch_codes[j]
                if isinstance(result, Exception):
                    logger.error(f"âŒ æœå°‹ {code} å¤±æ•—: {result}")
                    all_results[code] = {
                        'video_code': code,
                        'actresses': [],
                        'error': str(result)
                    }
                else:
                    all_results[code] = result
                
                if progress_callback:
                    status = "âœ… æˆåŠŸ" if not isinstance(result, Exception) else "âŒ å¤±æ•—"
                    progress_callback(f"{code}: {status}")
            
            # æ‰¹æ¬¡é–“æš«åœ
            if i + batch_size < len(video_codes):
                await asyncio.sleep(1.0)
        
        successful = sum(1 for r in all_results.values() if r.get('actresses'))
        logger.info(f"ğŸ‰ æ‰¹æ¬¡æœå°‹å®Œæˆ: {successful}/{len(video_codes)} æˆåŠŸ")
        
        return all_results
    
    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """ç²å–ç¶œåˆçµ±è¨ˆè³‡è¨Š"""
        # è¨ˆç®—å„æºæˆåŠŸç‡
        for source in DataSource:
            usage = self.stats['source_usage'][source]
            if usage > 0:
                # å¾å„çˆ¬èŸ²ç²å–æˆåŠŸç‡çµ±è¨ˆ
                scraper_stats = self.scrapers[source].get_comprehensive_stats()
                scraper_success = scraper_stats.get('scraper_stats', {}).get('successful_requests', 0)
                scraper_total = scraper_stats.get('scraper_stats', {}).get('total_requests', 1)
                self.stats['source_success_rate'][source] = (scraper_success / scraper_total * 100) if scraper_total > 0 else 0
        
        return {
            'unified_scraper_stats': self.stats,
            'cache_stats': self.cache_manager.get_stats(),
            'rate_limiter_stats': self.rate_limiter.get_stats(),
            'individual_scrapers': {
                source.value: scraper.get_comprehensive_stats()
                for source, scraper in self.scrapers.items()
            }
        }
    
    def clear_all_caches(self):
        """æ¸…ç©ºæ‰€æœ‰å¿«å–"""
        self.cache_manager.clear_cache()
        for scraper in self.scrapers.values():
            scraper.clear_cache()
        logger.info("ğŸ§¹ å·²æ¸…ç©ºæ‰€æœ‰çˆ¬èŸ²å¿«å–")
    
    async def health_check(self) -> Dict[str, Any]:
        """åŸ·è¡Œå¥åº·æª¢æŸ¥"""
        health_results = {}
        
        for source, scraper in self.scrapers.items():
            try:
                # å˜—è©¦æœå°‹ä¸€å€‹å¸¸è¦‹ç•ªè™Ÿé€²è¡Œå¥åº·æª¢æŸ¥
                test_result = await asyncio.wait_for(
                    scraper.search_video("test-001"),
                    timeout=10.0
                )
                health_results[source.value] = {
                    'status': 'healthy',
                    'response_time': '< 10s'
                }
            except Exception as e:
                health_results[source.value] = {
                    'status': 'unhealthy',
                    'error': str(e)
                }
        
        return {
            'timestamp': asyncio.get_event_loop().time(),
            'sources': health_results,
            'overall_health': all(
                result['status'] == 'healthy' 
                for result in health_results.values()
            )
        }