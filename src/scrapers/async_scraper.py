# -*- coding: utf-8 -*-
"""
éåŒæ­¥ç¶²è·¯çˆ¬èŸ²æ¨¡çµ„
æä¾›é«˜æ•ˆçš„ä½µç™¼ç¶²è·¯çˆ¬èŸ²åŠŸèƒ½
"""

import asyncio
import aiohttp
import logging
import time
from typing import Dict, List, Optional, Callable, Any, Tuple
from dataclasses import dataclass
from pathlib import Path
import json
import threading
from urllib.parse import urljoin, urlparse

from .encoding_utils import EncodingDetector, install_encoding_warning_filter
from .rate_limiter import RateLimiter
from .cache_manager import CacheManager

logger = logging.getLogger(__name__)

# å®‰è£ç·¨ç¢¼è­¦å‘Šéæ¿¾å™¨
install_encoding_warning_filter()


@dataclass
class ScrapingConfig:
    """çˆ¬èŸ²é…ç½®é¡"""
    max_concurrent: int = 3               # æœ€å¤§ä½µç™¼é€£ç·šæ•¸
    request_timeout: int = 30             # è«‹æ±‚è¶…æ™‚æ™‚é–“(ç§’)
    connect_timeout: int = 10             # é€£ç·šè¶…æ™‚æ™‚é–“(ç§’)
    total_timeout: int = 60               # ç¸½è¶…æ™‚æ™‚é–“(ç§’)
    max_retries: int = 3                  # æœ€å¤§é‡è©¦æ¬¡æ•¸
    backoff_factor: float = 2.0           # æŒ‡æ•¸é€€é¿å› å­
    enable_cache: bool = True             # å•Ÿç”¨å¿«å–
    cache_duration_hours: int = 24        # å¿«å–æŒçºŒæ™‚é–“(å°æ™‚)
    user_agent_rotation: bool = True      # å•Ÿç”¨ User-Agent è¼ªæ›¿
    respect_robots_txt: bool = True       # éµå®ˆ robots.txt
    max_redirects: int = 5                # æœ€å¤§é‡å®šå‘æ¬¡æ•¸


@dataclass
class ScrapingResult:
    """çˆ¬èŸ²çµæœé¡"""
    url: str
    success: bool
    data: Optional[Any] = None
    error: Optional[str] = None
    status_code: Optional[int] = None
    response_time: float = 0.0
    encoding: Optional[str] = None
    from_cache: bool = False


class AsyncWebScraper:
    """éåŒæ­¥ç¶²è·¯çˆ¬èŸ²é¡"""
    
    def __init__(self, config: ScrapingConfig = None, cache_manager: CacheManager = None):
        self.config = config or ScrapingConfig()
        self.encoding_detector = EncodingDetector()
        
        # åˆå§‹åŒ–é™æµå™¨å’Œå¿«å–ç®¡ç†å™¨
        self.rate_limiter = RateLimiter()
        self.cache_manager = cache_manager or CacheManager()
        
        # çµ±è¨ˆè³‡è¨Š
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'cache_hits': 0,
            'total_response_time': 0.0,
            'requests_by_domain': {},
            'encoding_stats': {}
        }
        
        # User-Agent æ± 
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0 Safari/537.36'
        ]
        self.current_ua_index = 0
        
        logger.info(f"ğŸš€ AsyncWebScraper å·²åˆå§‹åŒ– - æœ€å¤§ä½µç™¼: {self.config.max_concurrent}")
    
    def _get_headers(self) -> Dict[str, str]:
        """ç²å–è«‹æ±‚æ¨™é ­"""
        headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8,ja;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
        }
        
        # è¼ªæ›¿ User-Agent
        if self.config.user_agent_rotation:
            headers['User-Agent'] = self.user_agents[self.current_ua_index]
            self.current_ua_index = (self.current_ua_index + 1) % len(self.user_agents)
        else:
            headers['User-Agent'] = self.user_agents[0]
            
        return headers
    
    async def _make_request(self, session: aiohttp.ClientSession, url: str) -> ScrapingResult:
        """åŸ·è¡Œå–®å€‹è«‹æ±‚"""
        start_time = time.time()
        domain = urlparse(url).netloc
        
        try:
            # æª¢æŸ¥å¿«å–
            if self.config.enable_cache:
                cached_result = await self.cache_manager.get_async(url)
                if cached_result:
                    self.stats['cache_hits'] += 1
                    logger.debug(f"ğŸ“‹ å¾å¿«å–ç²å–: {url}")
                    return ScrapingResult(
                        url=url,
                        success=True,
                        data=cached_result,
                        from_cache=True,
                        response_time=time.time() - start_time
                    )
            
            # æ‡‰ç”¨é »ç‡é™åˆ¶
            await self.rate_limiter.wait_if_needed_async(domain)
            
            # ç™¼é€è«‹æ±‚
            timeout = aiohttp.ClientTimeout(
                total=self.config.total_timeout,
                connect=self.config.connect_timeout,
                sock_read=self.config.request_timeout
            )
            
            async with session.get(
                url, 
                headers=self._get_headers(),
                timeout=timeout,
                max_redirects=self.config.max_redirects
            ) as response:
                
                # è®€å–éŸ¿æ‡‰å…§å®¹
                content_bytes = await response.read()
                
                # è‡ªå‹•ç·¨ç¢¼æª¢æ¸¬
                decoded_content, encoding = self.encoding_detector.detect_and_decode(content_bytes)
                
                # æ›´æ–°çµ±è¨ˆ
                response_time = time.time() - start_time
                self._update_stats(domain, True, response_time, encoding)
                
                # å„²å­˜åˆ°å¿«å–
                if self.config.enable_cache and response.status == 200:
                    await self.cache_manager.set_async(url, decoded_content)
                
                return ScrapingResult(
                    url=url,
                    success=True,
                    data=decoded_content,
                    status_code=response.status,
                    response_time=response_time,
                    encoding=encoding
                )
                
        except asyncio.TimeoutError:
            error_msg = f"è«‹æ±‚è¶…æ™‚: {url}"
            logger.warning(f"â° {error_msg}")
            self._update_stats(domain, False, time.time() - start_time)
            return ScrapingResult(url=url, success=False, error=error_msg)
            
        except aiohttp.ClientError as e:
            error_msg = f"å®¢æˆ¶ç«¯éŒ¯èª¤: {e}"
            logger.warning(f"ğŸŒ {error_msg} - {url}")
            self._update_stats(domain, False, time.time() - start_time)
            return ScrapingResult(url=url, success=False, error=error_msg)
            
        except Exception as e:
            error_msg = f"æœªçŸ¥éŒ¯èª¤: {e}"
            logger.error(f"âŒ {error_msg} - {url}")
            self._update_stats(domain, False, time.time() - start_time)
            return ScrapingResult(url=url, success=False, error=error_msg)
    
    async def _make_request_with_retry(self, session: aiohttp.ClientSession, url: str) -> ScrapingResult:
        """å¸¶é‡è©¦æ©Ÿåˆ¶çš„è«‹æ±‚"""
        last_result = None
        
        for attempt in range(self.config.max_retries + 1):
            try:
                result = await self._make_request(session, url)
                
                if result.success:
                    return result
                    
                last_result = result
                
                # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€æ¬¡å˜—è©¦ï¼Œå‰‡ç­‰å¾…å¾Œé‡è©¦
                if attempt < self.config.max_retries:
                    wait_time = self.config.backoff_factor ** attempt
                    logger.info(f"â³ ç¬¬ {attempt + 1} æ¬¡å˜—è©¦å¤±æ•—ï¼Œç­‰å¾… {wait_time:.1f} ç§’å¾Œé‡è©¦: {url}")
                    await asyncio.sleep(wait_time)
                    
            except Exception as e:
                error_msg = f"é‡è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}"
                logger.error(f"âŒ {error_msg}")
                last_result = ScrapingResult(url=url, success=False, error=error_msg)
        
        logger.error(f"âŒ æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—äº†: {url}")
        return last_result or ScrapingResult(url=url, success=False, error="æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—")
    
    async def scrape_multiple(
        self, 
        urls: List[str], 
        progress_callback: Optional[Callable[[str], None]] = None
    ) -> List[ScrapingResult]:
        """ä½µç™¼çˆ¬å–å¤šå€‹URL"""
        
        if not urls:
            return []
            
        logger.info(f"ğŸš€ é–‹å§‹ä½µç™¼çˆ¬å– {len(urls)} å€‹URL")
        
        # å‰µå»ºä¿¡è™Ÿé‡æ§åˆ¶ä½µç™¼æ•¸
        semaphore = asyncio.Semaphore(self.config.max_concurrent)
        
        async def scrape_with_semaphore(session: aiohttp.ClientSession, url: str) -> ScrapingResult:
            async with semaphore:
                result = await self._make_request_with_retry(session, url)
                if progress_callback:
                    status = "âœ… æˆåŠŸ" if result.success else f"âŒ å¤±æ•—: {result.error}"
                    progress_callback(f"{url}: {status}")
                return result
        
        # å‰µå»ºæœƒè©±ä¸¦åŸ·è¡Œæ‰€æœ‰è«‹æ±‚
        connector = aiohttp.TCPConnector(
            limit=self.config.max_concurrent * 2,  # é€£ç·šæ± å¤§å°
            limit_per_host=self.config.max_concurrent,
            ttl_dns_cache=300,  # DNSå¿«å–5åˆ†é˜
            use_dns_cache=True
        )
        
        async with aiohttp.ClientSession(connector=connector) as session:
            tasks = [scrape_with_semaphore(session, url) for url in urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # è™•ç†ç•°å¸¸çµæœ
            processed_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    error_result = ScrapingResult(
                        url=urls[i], 
                        success=False, 
                        error=str(result)
                    )
                    processed_results.append(error_result)
                else:
                    processed_results.append(result)
        
        successful = sum(1 for r in processed_results if r.success)
        logger.info(f"âœ… çˆ¬å–å®Œæˆ: {successful}/{len(urls)} æˆåŠŸ")
        
        return processed_results
    
    def scrape_multiple_sync(
        self, 
        urls: List[str], 
        progress_callback: Optional[Callable[[str], None]] = None
    ) -> List[ScrapingResult]:
        """åŒæ­¥ä»‹é¢çš„ä½µç™¼çˆ¬å–"""
        try:
            # å˜—è©¦ç²å–ç¾æœ‰çš„äº‹ä»¶å¾ªç’°
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # å¦‚æœå·²ç¶“åœ¨äº‹ä»¶å¾ªç’°ä¸­ï¼Œä½¿ç”¨æ–°çš„äº‹ä»¶å¾ªç’°
                import threading
                import concurrent.futures
                
                def run_in_thread():
                    new_loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(new_loop)
                    try:
                        return new_loop.run_until_complete(
                            self.scrape_multiple(urls, progress_callback)
                        )
                    finally:
                        new_loop.close()
                
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(run_in_thread)
                    return future.result()
            else:
                return loop.run_until_complete(
                    self.scrape_multiple(urls, progress_callback)
                )
        except RuntimeError:
            # æ²’æœ‰äº‹ä»¶å¾ªç’°ï¼Œå‰µå»ºæ–°çš„
            return asyncio.run(self.scrape_multiple(urls, progress_callback))
    
    def _update_stats(self, domain: str, success: bool, response_time: float, encoding: str = None):
        """æ›´æ–°çµ±è¨ˆè³‡è¨Š"""
        self.stats['total_requests'] += 1
        
        if success:
            self.stats['successful_requests'] += 1
        else:
            self.stats['failed_requests'] += 1
            
        self.stats['total_response_time'] += response_time
        
        # åŸŸåçµ±è¨ˆ
        if domain not in self.stats['requests_by_domain']:
            self.stats['requests_by_domain'][domain] = {'total': 0, 'success': 0}
        self.stats['requests_by_domain'][domain]['total'] += 1
        if success:
            self.stats['requests_by_domain'][domain]['success'] += 1
            
        # ç·¨ç¢¼çµ±è¨ˆ
        if encoding:
            if encoding not in self.stats['encoding_stats']:
                self.stats['encoding_stats'][encoding] = 0
            self.stats['encoding_stats'][encoding] += 1
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–çˆ¬èŸ²çµ±è¨ˆè³‡è¨Š"""
        total_requests = self.stats['total_requests']
        
        avg_response_time = (
            self.stats['total_response_time'] / total_requests 
            if total_requests > 0 else 0
        )
        
        success_rate = (
            self.stats['successful_requests'] / total_requests * 100 
            if total_requests > 0 else 0
        )
        
        return {
            **self.stats,
            'success_rate': f"{success_rate:.1f}%",
            'average_response_time': f"{avg_response_time:.2f}s",
            'cache_hit_rate': f"{(self.stats['cache_hits'] / total_requests * 100):.1f}%" if total_requests > 0 else "0%",
            'encoding_detector_stats': self.encoding_detector.get_stats(),
            'rate_limiter_stats': self.rate_limiter.get_stats(),
            'cache_manager_stats': self.cache_manager.get_stats()
        }
    
    def clear_cache(self):
        """æ¸…ç©ºå¿«å–"""
        self.cache_manager.clear_cache()
        logger.info("ğŸ§¹ å·²æ¸…ç©ºçˆ¬èŸ²å¿«å–")
    
    def reset_stats(self):
        """é‡ç½®çµ±è¨ˆè³‡è¨Š"""
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'cache_hits': 0,
            'total_response_time': 0.0,
            'requests_by_domain': {},
            'encoding_stats': {}
        }
        logger.info("ğŸ“Š å·²é‡ç½®çˆ¬èŸ²çµ±è¨ˆè³‡è¨Š")


class BatchWebScraper:
    """æ‰¹æ¬¡ç¶²è·¯çˆ¬èŸ² - é‡å°å¤§é‡URLçš„å„ªåŒ–ç‰ˆæœ¬"""
    
    def __init__(self, config: ScrapingConfig = None, batch_size: int = 20):
        self.config = config or ScrapingConfig()
        self.batch_size = batch_size
        self.scraper = AsyncWebScraper(config)
        
    def scrape_in_batches(
        self, 
        urls: List[str], 
        progress_callback: Optional[Callable[[str], None]] = None
    ) -> List[ScrapingResult]:
        """åˆ†æ‰¹çˆ¬å–å¤§é‡URL"""
        
        if not urls:
            return []
            
        total_batches = (len(urls) + self.batch_size - 1) // self.batch_size
        all_results = []
        
        logger.info(f"ğŸ“¦ é–‹å§‹åˆ†æ‰¹çˆ¬å– {len(urls)} å€‹URLï¼Œåˆ†ç‚º {total_batches} æ‰¹")
        
        for i in range(0, len(urls), self.batch_size):
            batch_urls = urls[i:i + self.batch_size]
            batch_num = (i // self.batch_size) + 1
            
            if progress_callback:
                progress_callback(f"è™•ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_urls)} å€‹URL)...")
            
            logger.info(f"ğŸ“¦ è™•ç†æ‰¹æ¬¡ {batch_num}/{total_batches}")
            
            # çˆ¬å–ç•¶å‰æ‰¹æ¬¡
            batch_results = self.scraper.scrape_multiple_sync(
                batch_urls, 
                progress_callback
            )
            all_results.extend(batch_results)
            
            # æ‰¹æ¬¡é–“æš«åœ
            if i + self.batch_size < len(urls):
                pause_time = 2.0  # æ‰¹æ¬¡é–“æš«åœ2ç§’
                logger.info(f"â¸ï¸ æ‰¹æ¬¡é–“æš«åœ {pause_time} ç§’...")
                time.sleep(pause_time)
        
        successful = sum(1 for r in all_results if r.success)
        logger.info(f"ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡å®Œæˆ: {successful}/{len(urls)} æˆåŠŸ")
        
        return all_results