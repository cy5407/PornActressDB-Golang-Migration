# -*- coding: utf-8 -*-
"""
å®‰å…¨çš„ JAVDB æœå°‹å™¨ - æ•´åˆåçˆ¬èŸ²ç­–ç•¥
"""
import time
import random
import httpx
from bs4 import BeautifulSoup
import logging
from pathlib import Path
import json
from datetime import datetime, date
from typing import Dict, List, Optional, Any
import threading
from urllib.parse import quote, urljoin

logger = logging.getLogger(__name__)


class SafeJAVDBSearcher:
    """å®‰å…¨çš„ JAVDB æœå°‹å™¨é¡åˆ¥"""
    
    def __init__(self, cache_dir: str = None):
        self.cache_dir = Path(cache_dir) if cache_dir else Path(__file__).parent.parent.parent / 'data'
        self.cache_file = self.cache_dir / 'javdb_search_cache.json'
        self.stats_file = self.cache_dir / 'javdb_stats.json'
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # è¼‰å…¥å¿«å–å’Œçµ±è¨ˆè³‡æ–™
        self.load_cache()
        self.load_stats()
        
        # å®‰å…¨åƒæ•¸è¨­å®š
        self.request_count = 0
        self.max_requests_per_session = 25  # é™ä½sessionè«‹æ±‚æ•¸é™åˆ¶
        self.daily_limit = 80  # é™ä½æ¯æ—¥é™åˆ¶æ›´å®‰å…¨
        self.min_delay = 3.0  # å¢åŠ æœ€å°å»¶é²
        self.max_delay = 7.0  # å¢åŠ æœ€å¤§å»¶é²
        
        # æª¢æŸ¥ç•¶æ—¥çµ±è¨ˆ
        self._check_daily_reset()
        
        # ç·šç¨‹é–ä¿è­·å…±äº«è³‡æº
        self._lock = threading.Lock()
        
        # åˆå§‹åŒ–æœƒè©±
        self.create_session()
        
        logger.info(f"ğŸ›¡ï¸ JAVDB å®‰å…¨æœå°‹å™¨å·²åˆå§‹åŒ– - æ¯æ—¥é™åˆ¶: {self.daily_limit}")

    def _check_daily_reset(self):
        """æª¢æŸ¥æ˜¯å¦éœ€è¦é‡ç½®æ¯æ—¥è¨ˆæ•¸"""
        current_date = date.today().isoformat()
        if self.stats.get('last_date') != current_date:
            self.stats['last_date'] = current_date
            self.stats['today_count'] = 0
            self.save_stats()
            logger.info(f"ğŸ“… æ¯æ—¥çµ±è¨ˆå·²é‡ç½® - {current_date}")

    def load_cache(self):
        """è¼‰å…¥å¿«å–è³‡æ–™"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    self.cache = json.load(f)
                logger.debug(f"ğŸ“¦ å·²è¼‰å…¥ {len(self.cache)} å€‹å¿«å–é …ç›®")
            except Exception as e:
                logger.warning(f"è¼‰å…¥å¿«å–å¤±æ•—: {e}")
                self.cache = {}
        else:
            self.cache = {}

    def save_cache(self):
        """å„²å­˜å¿«å–è³‡æ–™"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
            logger.debug(f"ğŸ’¾ å·²å„²å­˜ {len(self.cache)} å€‹å¿«å–é …ç›®")
        except Exception as e:
            logger.error(f"å„²å­˜å¿«å–å¤±æ•—: {e}")

    def load_stats(self):
        """è¼‰å…¥çµ±è¨ˆè³‡æ–™"""
        if self.stats_file.exists():
            try:
                with open(self.stats_file, 'r', encoding='utf-8') as f:
                    self.stats = json.load(f)
            except Exception as e:
                logger.warning(f"è¼‰å…¥çµ±è¨ˆå¤±æ•—: {e}")
                self.stats = {}
        else:
            self.stats = {}
        
        # ç¢ºä¿å¿…è¦çš„çµ±è¨ˆæ¬„ä½å­˜åœ¨
        if 'today_count' not in self.stats:
            self.stats['today_count'] = 0
        if 'total_requests' not in self.stats:
            self.stats['total_requests'] = 0
        if 'successful_searches' not in self.stats:
            self.stats['successful_searches'] = 0

    def save_stats(self):
        """å„²å­˜çµ±è¨ˆè³‡æ–™"""
        try:
            with open(self.stats_file, 'w', encoding='utf-8') as f:
                json.dump(self.stats, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"å„²å­˜çµ±è¨ˆå¤±æ•—: {e}")

    def create_session(self):
        """å»ºç«‹æ¨¡æ“¬çœŸå¯¦ç€è¦½å™¨çš„ session"""
        user_agents = [
            # Chrome on Windows
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            # Chrome on macOS
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            # Safari on macOS
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
            # Firefox on Windows
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0',
            # Edge on Windows
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0'        ]
        
        headers = {
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,ja;q=0.8,en-US;q=0.7,en;q=0.6',
            # æš«æ™‚ç§»é™¤ br å£“ç¸®ä»¥é¿å…è§£ç¢¼å•é¡Œ
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
        }
          # éš¨æ©Ÿæ·»åŠ ä¸€äº›å¯é¸æ¨™é ­
        if random.choice([True, False]):
            headers['DNT'] = '1'
        
        if random.choice([True, False]):
            headers['Referer'] = 'https://www.google.com/'
        
        self.session = httpx.Client(
            headers=headers,
            follow_redirects=True,
            timeout=30.0,
            limits=httpx.Limits(max_connections=1, max_keepalive_connections=1),
            # ç¢ºä¿æ­£ç¢ºè™•ç†å£“ç¸®å›æ‡‰
            default_encoding='utf-8'
        )
        
        self.request_count = 0
        logger.debug(f"ğŸ”„ å·²å»ºç«‹æ–°çš„æœƒè©± - User-Agent: {headers['User-Agent'][:50]}...")

    def safe_request(self, url: str, retry_count: int = 0) -> Optional[httpx.Response]:
        """å®‰å…¨çš„ HTTP è«‹æ±‚æ–¹æ³•"""
        with self._lock:
            # æª¢æŸ¥æ¯æ—¥é™åˆ¶
            self._check_daily_reset()
            if self.stats['today_count'] >= self.daily_limit:
                logger.warning(f"âš ï¸ å·²é”æ¯æ—¥ JAVDB è«‹æ±‚é™åˆ¶ ({self.daily_limit})")
                return None
            
            # æª¢æŸ¥ session è«‹æ±‚æ¬¡æ•¸
            if self.request_count >= self.max_requests_per_session:
                logger.info("ğŸ”„ é‡æ–°å»ºç«‹ JAVDB session")
                self.create_session()
            
            try:
                # æ™ºèƒ½éš¨æ©Ÿå»¶é²
                base_delay = random.uniform(self.min_delay, self.max_delay)
                # å¦‚æœæ˜¯é‡è©¦ï¼Œå¢åŠ é¡å¤–å»¶é²
                if retry_count > 0:
                    base_delay += retry_count * 2.0
                
                logger.debug(f"â±ï¸ ç­‰å¾… {base_delay:.1f} ç§’...")
                time.sleep(base_delay)
                
                # åŸ·è¡Œè«‹æ±‚
                response = self.session.get(url)
                self.request_count += 1
                self.stats['today_count'] += 1
                self.stats['total_requests'] += 1
                
                # è™•ç†ä¸åŒçš„ HTTP ç‹€æ…‹ç¢¼
                if response.status_code == 429:  # Too Many Requests
                    if retry_count < 3:
                        wait_time = 60 + random.uniform(30, 90)  # 1-2.5åˆ†é˜
                        logger.warning(f"âš ï¸ æ”¶åˆ° 429 éŒ¯èª¤ï¼Œç­‰å¾… {wait_time:.1f} ç§’å¾Œé‡è©¦...")
                        time.sleep(wait_time)
                        return self.safe_request(url, retry_count + 1)
                    else:
                        logger.error("âŒ é‡è©¦æ¬¡æ•¸éå¤šï¼Œæ”¾æ£„è«‹æ±‚")
                        return None
                
                elif response.status_code == 403:  # Forbidden
                    logger.warning("âš ï¸ æ”¶åˆ° 403 éŒ¯èª¤ï¼Œå¯èƒ½è¢«æš«æ™‚å°é–")
                    if retry_count < 2:
                        # é‡æ–°å»ºç«‹ session ä¸¦ç­‰å¾…æ›´é•·æ™‚é–“
                        self.create_session()
                        wait_time = 120 + random.uniform(60, 180)  # 2-5åˆ†é˜
                        logger.info(f"â³ ç­‰å¾… {wait_time:.1f} ç§’å¾Œé‡è©¦...")
                        time.sleep(wait_time)
                        return self.safe_request(url, retry_count + 1)
                    return None
                
                elif response.status_code != 200:
                    logger.warning(f"âš ï¸ JAVDB è«‹æ±‚å¤±æ•—: {response.status_code}")
                    return None
                
                logger.debug(f"âœ… JAVDB è«‹æ±‚æˆåŠŸ: {response.status_code}")
                return response
                
            except httpx.TimeoutException:
                logger.warning("â° JAVDB è«‹æ±‚è¶…æ™‚")
                if retry_count < 2:
                    return self.safe_request(url, retry_count + 1)
                return None
                
            except httpx.ConnectError:
                logger.warning("ğŸ”Œ JAVDB é€£ç·šå¤±æ•—")
                if retry_count < 2:
                    time.sleep(10 + retry_count * 5)
                    return self.safe_request(url, retry_count + 1)
                return None
                
            except Exception as e:
                logger.error(f"âŒ JAVDB è«‹æ±‚éç¨‹ä¸­å‡ºéŒ¯: {e}")
                if retry_count < 1:
                    time.sleep(5)
                    return self.safe_request(url, retry_count + 1)
                return None

    def search_javdb(self, video_id: str) -> Optional[Dict[str, Any]]:
        """åœ¨ JAVDB æœå°‹å½±ç‰‡è³‡è¨Š"""
        if not video_id:
            return None
              # æª¢æŸ¥å¿«å–
        cache_key = f"javdb_{video_id.upper()}"
        if cache_key in self.cache:
            logger.debug(f"ğŸ“‹ å¾å¿«å–å–å¾— {video_id} çš„ JAVDB è³‡æ–™")
            return self.cache[cache_key]
        
        try:
            # æ§‹å»ºæœå°‹ URL
            search_url = f"https://javdb.com/search?q={quote(video_id)}&f=all"
              # åŸ·è¡Œæœå°‹
            response = self.safe_request(search_url)
            if not response:
                return None
            
            # JAVDB ä½¿ç”¨æ¨™æº– UTF-8 ç·¨ç¢¼ï¼Œä¸éœ€è¦ç‰¹æ®Šè™•ç†
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å°‹æ‰¾å½±ç‰‡é€£çµ - ä½¿ç”¨å¯¦éš›çš„JAVDBçµæ§‹
            video_links = soup.select('a[href*="/v/"]')
            
            if not video_links:
                logger.info(f"ğŸ” JAVDB æœªæ‰¾åˆ°ç•ªè™Ÿ {video_id} çš„çµæœ")
                return None
            
            logger.debug(f"ğŸ¬ æ‰¾åˆ° {len(video_links)} å€‹å½±ç‰‡é€£çµ")
            
            # å°‹æ‰¾æœ€åŒ¹é…çš„çµæœ
            best_match_url = None
            
            # æª¢æŸ¥æ¯å€‹é€£çµå°æ‡‰çš„å½±ç‰‡ï¼Œçœ‹æ˜¯å¦åŒ¹é…ç•ªè™Ÿ
            for link in video_links:
                href = link.get('href')
                if not href:
                    continue
                
                # æª¢æŸ¥é€£çµå‘¨åœçš„æ–‡å­—æˆ–æ¨™é¡Œæ˜¯å¦åŒ…å«ç•ªè™Ÿ
                link_text = link.get_text(strip=True)
                title_attr = link.get('title', '')
                
                # æª¢æŸ¥æ˜¯å¦åŒ¹é…
                text_to_check = f"{link_text} {title_attr}".upper()
                if video_id.upper() in text_to_check:
                    best_match_url = href
                    logger.debug(f"ğŸ¯ æ‰¾åˆ°åŒ¹é…çš„å½±ç‰‡é€£çµ: {href} (æ–‡å­—: {link_text})")
                    break
              # å¦‚æœæ²’æœ‰æ‰¾åˆ°å®Œå…¨åŒ¹é…çš„ï¼Œä½¿ç”¨ç¬¬ä¸€å€‹çµæœ
            if not best_match_url:
                best_match_url = video_links[0].get('href')
                logger.debug(f"ğŸ² ä½¿ç”¨ç¬¬ä¸€å€‹æœå°‹çµæœ: {best_match_url}")
            
            if not best_match_url:
                logger.warning(f"âš ï¸ ç„¡æ³•ç²å– {video_id} çš„è©³æƒ…é é¢é€£çµ")
                return None
            
            detail_url = urljoin('https://javdb.com', best_match_url)
            
            # è¨ªå•è©³æƒ…é é¢
            detail_response = self.safe_request(detail_url)
            if not detail_response:
                return None
            
            # è§£æè©³æƒ…é é¢ - ä½¿ç”¨ç·¨ç¢¼å¢å¼·å™¨
            info = self._parse_detail_page(detail_response, video_id, detail_url)
            
            if info:
                # å„²å­˜åˆ°å¿«å–
                self.cache[cache_key] = info
                self.save_cache()
                self.stats['successful_searches'] += 1
                self.save_stats()
                
                logger.info(f"âœ… JAVDB æ‰¾åˆ°ç•ªè™Ÿ {video_id} çš„è³‡æ–™")
                return info
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ æœå°‹ {video_id} æ™‚å‡ºéŒ¯: {e}")
            return None

    def _parse_detail_page(self, response: httpx.Response, video_id: str, url: str) -> Optional[Dict[str, Any]]:
        """è§£æ JAVDB è©³æƒ…é é¢"""
        try:
            # JAVDB ä½¿ç”¨æ¨™æº– UTF-8 ç·¨ç¢¼ï¼Œä¸éœ€è¦ç‰¹æ®Šè™•ç†
            soup = BeautifulSoup(response.text, 'html.parser')
            
            if soup is None:
                logger.warning(f"ç„¡æ³•è§£æ JAVDB è©³æƒ…é é¢: {url}")
                return None
            
            info = {
                'code': video_id.upper(),
                'source': 'JAVDB (å®‰å…¨å¢å¼·ç‰ˆ)',
                'actresses': [],
                'studio': None,
                'studio_code': None,
                'release_date': None,
                'title': None,
                'duration': None,
                'director': None,
                'series': None,
                'rating': None,
                'categories': []
            }
            
            # æå–æ¨™é¡Œ
            title_element = soup.select_one('h2.title')
            if title_element:
                info['title'] = title_element.text.strip()
              # æå–è©³ç´°è³‡è¨Š - é©é…æ–°çš„ HTML çµæ§‹
            info_panels = soup.select('.panel-block')
            
            for panel in info_panels:
                strong_element = panel.select_one('strong')
                if not strong_element:
                    continue
                    
                label = strong_element.text.strip().rstrip(':ï¼š')
                
                # å°æ–¼æ¼”å“¡è³‡è¨Šï¼Œç›´æ¥åœ¨åŒä¸€å€‹ panel ä¸­å°‹æ‰¾
                if label == 'æ¼”å“¡':
                    # æå–å¥³å„ªåç¨±ï¼ˆåªå–å¥³æ€§æ¼”å“¡ï¼‰
                    actresses = []
                    # å°‹æ‰¾æ¼”å“¡é€£çµå’Œæ€§åˆ¥ç¬¦è™Ÿ
                    actress_links = panel.select('a[href*="/actors/"]')
                    for link in actress_links:
                        # æª¢æŸ¥ç·Šè·Ÿè‘—çš„æ€§åˆ¥ç¬¦è™Ÿ
                        next_element = link.find_next_sibling()
                        if (next_element and 
                            next_element.name == 'strong' and 
                            next_element.get('class') and 
                            'symbol' in next_element.get('class') and 
                            'female' in next_element.get('class')):
                            actress_name = link.text.strip()
                            if actress_name:
                                actresses.append(actress_name)
                        # å‚™ç”¨æª¢æŸ¥ï¼šå¦‚æœæ²’æœ‰ classï¼Œæª¢æŸ¥æ–‡å­—å…§å®¹
                        elif (next_element and next_element.name == 'strong' and 'â™€' in next_element.text):
                            actress_name = link.text.strip()
                            if actress_name:
                                actresses.append(actress_name)
                    info['actresses'] = actresses
                    continue
                
                # å–å¾—å€¼å®¹å™¨ï¼ˆéæ¼”å“¡æ¬„ä½ï¼‰
                value_element = panel.select_one('.value')
                if not value_element:
                    continue
                
                elif label == 'ç‰‡å•†':
                    # æå–ç‰‡å•†
                    maker_link = value_element.select_one('a[href*="/makers/"]')
                    if maker_link:
                        info['studio'] = maker_link.text.strip()
                
                elif label == 'æ—¥æœŸ':
                    # æå–ç™¼è¡Œæ—¥æœŸ
                    date_text = value_element.text.strip()
                    if date_text:
                        info['release_date'] = date_text
                
                elif label == 'æ™‚é•·':
                    # æå–æ™‚é•·
                    duration_text = value_element.text.strip()
                    if duration_text:
                        info['duration'] = duration_text
                
                elif label == 'å°æ¼”':
                    # æå–å°æ¼”
                    director_link = value_element.select_one('a')
                    if director_link:
                        info['director'] = director_link.text.strip()
                
                elif label == 'ç³»åˆ—':
                    # æå–ç³»åˆ—
                    series_link = value_element.select_one('a')
                    if series_link:
                        info['series'] = series_link.text.strip()
                
                elif label == 'è©•åˆ†':
                    # æå–è©•åˆ†
                    rating_text = value_element.text.strip()
                    # æå–æ•¸å­—è©•åˆ† (å¦‚ "4.26åˆ†, ç”±564äººè©•åƒ¹")
                    import re
                    rating_match = re.search(r'(\d+\.?\d*)åˆ†', rating_text)
                    if rating_match:
                        info['rating'] = float(rating_match.group(1))
                
                elif label == 'é¡åˆ¥':
                    # æå–é¡åˆ¥
                    category_links = value_element.select('a')
                    info['categories'] = [link.text.strip() for link in category_links]
            
            # å˜—è©¦å¾ç•ªè™Ÿæ¨æ¸¬ç‰‡å•†ä»£ç¢¼
            if not info['studio_code'] and video_id:
                info['studio_code'] = self._extract_studio_code_from_number(video_id)
            
            # ç¢ºä¿è‡³å°‘æœ‰å¥³å„ªè³‡è¨Šæ‰è¿”å›çµæœ
            if info['actresses']:
                return info
            else:
                logger.warning(f"âš ï¸ JAVDB é é¢ä¸­æœªæ‰¾åˆ° {video_id} çš„å¥³å„ªè³‡è¨Š")
                return None
                
        except Exception as e:
            logger.error(f"âŒ è§£æ JAVDB è©³æƒ…é é¢æ™‚å‡ºéŒ¯: {e}")
            return None

    def _extract_studio_code_from_number(self, code: str) -> Optional[str]:
        """å¾ç•ªè™Ÿä¸­æå–ç‰‡å•†ä»£ç¢¼"""
        import re
        if not code:
            return None
        
        # æå–å­—æ¯éƒ¨åˆ†ä½œç‚ºç‰‡å•†ä»£ç¢¼
        match = re.match(r'^([A-Z]+)', code.upper())
        if match:
            return match.group(1)
        return None

    def get_stats(self) -> Dict[str, Any]:
        """ç²å–æœå°‹çµ±è¨ˆè³‡è¨Š"""
        self._check_daily_reset()
        return {
            'today_count': self.stats.get('today_count', 0),
            'daily_limit': self.daily_limit,
            'total_requests': self.stats.get('total_requests', 0),
            'successful_searches': self.stats.get('successful_searches', 0),
            'cache_entries': len(self.cache),
            'session_requests': self.request_count,
            'session_limit': self.max_requests_per_session,
            'last_date': self.stats.get('last_date')
        }

    def clear_cache(self):
        """æ¸…ç©ºå¿«å–"""
        self.cache.clear()
        try:
            if self.cache_file.exists():
                self.cache_file.unlink()
            logger.info("ğŸ§¹ å·²æ¸…ç©º JAVDB å¿«å–")
        except Exception as e:
            logger.warning(f"æ¸…ç©º JAVDB å¿«å–å¤±æ•—: {e}")

    def __del__(self):
        """ææ§‹å‡½æ•¸ - æ¸…ç†è³‡æº"""
        try:
            if hasattr(self, 'session'):
                self.session.close()
            if hasattr(self, 'cache'):
                self.save_cache()
            if hasattr(self, 'stats'):
                self.save_stats()
        except:
            pass