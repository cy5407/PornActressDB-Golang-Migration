# -*- coding: utf-8 -*-
"""
å®‰å…¨æœå°‹å™¨æ¨¡çµ„ - é˜²æ­¢è¢«ç¶²ç«™å°é–çš„ç¶²è·¯æœå°‹å¢å¼·åŠŸèƒ½
"""
import time
import random
import logging
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Callable, Any
from pathlib import Path
import json
import threading
from dataclasses import dataclass, asdict

logger = logging.getLogger(__name__)


@dataclass
class RequestConfig:
    """è«‹æ±‚é…ç½®é¡"""
    min_interval: float = 1.0  # æœ€å°è«‹æ±‚é–“éš”(ç§’)
    max_interval: float = 3.0  # æœ€å¤§è«‹æ±‚é–“éš”(ç§’)
    enable_cache: bool = True  # å•Ÿç”¨å¿«å–
    cache_duration: int = 86400  # å¿«å–æŒçºŒæ™‚é–“(ç§’, é è¨­24å°æ™‚)
    max_retries: int = 3  # æœ€å¤§é‡è©¦æ¬¡æ•¸
    backoff_factor: float = 2.0  # æŒ‡æ•¸é€€é¿å› å­
    rotate_headers: bool = True  # è¼ªæ›¿è«‹æ±‚æ¨™é ­


@dataclass 
class CacheEntry:
    """å¿«å–é …ç›®"""
    data: Any
    timestamp: float
    url: str
    request_hash: str


class SafeSearcher:
    """å®‰å…¨æœå°‹å™¨ - é˜²æ­¢IPè¢«å°é–çš„æ™ºèƒ½æœå°‹å™¨"""
    
    def __init__(self, config: RequestConfig = None, cache_file: str = None):
        self.config = config or RequestConfig()
        self.last_request_time = 0.0
        self._request_lock = threading.Lock()
        
        # åˆå§‹åŒ–å¿«å–ç³»çµ±
        self.cache_file = cache_file or str(Path(__file__).parent.parent.parent / 'cache' / 'search_cache.json')
        self.cache: Dict[str, CacheEntry] = {}
        self._load_cache()
        
        # åˆå§‹åŒ–ç€è¦½å™¨æ¨™é ­æ± 
        self.browser_headers = self._init_browser_headers()
        self.current_header_index = 0
        
        logger.info(f"ğŸ›¡ï¸ å®‰å…¨æœå°‹å™¨å·²å•Ÿå‹• - é–“éš”: {self.config.min_interval}-{self.config.max_interval}s")

    def _init_browser_headers(self) -> List[Dict[str, str]]:
        """åˆå§‹åŒ–çœŸå¯¦ç€è¦½å™¨æ¨™é ­æ± """
        return [
            {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8,ja;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"Windows"'
            },
            {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-TW,zh;q=0.8,en-US;q=0.5,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none'
            },
            {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"macOS"'
            },
            {
                'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"Linux"'
            },
            {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Microsoft Edge";v="120"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"Windows"'
            }
        ]

    def get_headers(self) -> Dict[str, str]:
        """ç²å–ç•¶å‰è«‹æ±‚æ¨™é ­"""
        if not self.config.rotate_headers:
            return self.browser_headers[0]
            
        # è¼ªæ›¿æ¨™é ­
        headers = self.browser_headers[self.current_header_index].copy()
        self.current_header_index = (self.current_header_index + 1) % len(self.browser_headers)
        
        # éš¨æ©ŸåŒ–éƒ¨åˆ†æ¨™é ­
        headers['Cache-Control'] = random.choice(['no-cache', 'max-age=0', 'no-store'])
        
        return headers

    def _wait_for_next_request(self):
        """æ™ºèƒ½è«‹æ±‚é–“éš”æ§åˆ¶"""
        with self._request_lock:
            current_time = time.time()
            elapsed = current_time - self.last_request_time
            
            # è¨ˆç®—éš¨æ©Ÿå»¶é²
            min_wait = self.config.min_interval
            max_wait = self.config.max_interval
            wait_time = random.uniform(min_wait, max_wait)
            
            if elapsed < wait_time:
                sleep_time = wait_time - elapsed
                logger.debug(f"â±ï¸ ç­‰å¾… {sleep_time:.2f} ç§’å¾Œç™¼é€ä¸‹ä¸€å€‹è«‹æ±‚...")
                time.sleep(sleep_time)
            
            self.last_request_time = time.time()

    def _generate_cache_key(self, url: str, params: dict = None) -> str:
        """ç”Ÿæˆå¿«å–éµå€¼"""
        cache_string = f"{url}_{str(params or {})}"
        return hashlib.md5(cache_string.encode('utf-8')).hexdigest()

    def _is_cache_valid(self, cache_entry: CacheEntry) -> bool:
        """æª¢æŸ¥å¿«å–æ˜¯å¦æœ‰æ•ˆ"""
        if not self.config.enable_cache:
            return False
            
        current_time = time.time()
        return (current_time - cache_entry.timestamp) < self.config.cache_duration

    def _load_cache(self):
        """è¼‰å…¥å¿«å–è³‡æ–™"""
        if not self.config.enable_cache:
            return
            
        try:
            cache_path = Path(self.cache_file)
            if cache_path.exists():
                with open(cache_path, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                    
                # è½‰æ›ç‚º CacheEntry ç‰©ä»¶
                for key, value in cache_data.items():
                    self.cache[key] = CacheEntry(**value)
                    
                # æ¸…ç†éæœŸå¿«å–
                self._cleanup_expired_cache()
                logger.info(f"ğŸ“¦ å·²è¼‰å…¥ {len(self.cache)} å€‹å¿«å–é …ç›®")
        except Exception as e:
            logger.warning(f"è¼‰å…¥å¿«å–å¤±æ•—: {e}")
            self.cache = {}

    def _save_cache(self):
        """ä¿å­˜å¿«å–è³‡æ–™"""
        if not self.config.enable_cache:
            return
            
        try:
            cache_path = Path(self.cache_file)
            cache_path.parent.mkdir(parents=True, exist_ok=True)
            
            # è½‰æ›ç‚ºå¯åºåˆ—åŒ–çš„æ ¼å¼ï¼Œéæ¿¾æ‰ BeautifulSoup ç‰©ä»¶
            from bs4 import BeautifulSoup
            cache_data = {}
            
            for key, entry in self.cache.items():
                # æª¢æŸ¥è³‡æ–™æ˜¯å¦ç‚º BeautifulSoup ç‰©ä»¶
                if isinstance(entry.data, BeautifulSoup):
                    logger.debug(f"è·³é BeautifulSoup ç‰©ä»¶å¿«å–: {entry.url}")
                    continue
                    
                try:
                    # æ¸¬è©¦æ˜¯å¦å¯åºåˆ—åŒ–
                    json.dumps(entry.data, ensure_ascii=False)
                    cache_data[key] = asdict(entry)
                except (TypeError, ValueError):
                    logger.debug(f"è·³éä¸å¯åºåˆ—åŒ–è³‡æ–™: {entry.url}")
                    continue
            
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
                
            logger.debug(f"ğŸ’¾ å·²å„²å­˜ {len(cache_data)} å€‹å¿«å–é …ç›® (è·³é {len(self.cache) - len(cache_data)} å€‹ä¸å¯åºåˆ—åŒ–é …ç›®)")
        except Exception as e:
            logger.warning(f"ä¿å­˜å¿«å–å¤±æ•—: {e}")

    def _cleanup_expired_cache(self):
        """æ¸…ç†éæœŸå¿«å–"""
        if not self.config.enable_cache:
            return
            
        expired_keys = []
        current_time = time.time()
        
        for key, entry in self.cache.items():
            if (current_time - entry.timestamp) >= self.config.cache_duration:
                expired_keys.append(key)
        
        for key in expired_keys:
            del self.cache[key]
            
        if expired_keys:
            logger.info(f"ğŸ§¹ å·²æ¸…ç† {len(expired_keys)} å€‹éæœŸå¿«å–é …ç›®")

    def get_from_cache(self, url: str, params: dict = None) -> Optional[Any]:
        """å¾å¿«å–ç²å–è³‡æ–™"""
        if not self.config.enable_cache:
            return None
            
        cache_key = self._generate_cache_key(url, params)
        entry = self.cache.get(cache_key)
        
        if entry and self._is_cache_valid(entry):
            logger.debug(f"ğŸ“‹ å¾å¿«å–ç²å–: {url}")
            return entry.data
            
        return None

    def save_to_cache(self, url: str, data: Any, params: dict = None):
        """ä¿å­˜è³‡æ–™åˆ°å¿«å–"""
        if not self.config.enable_cache:
            return
            
        # æª¢æŸ¥è³‡æ–™æ˜¯å¦å¯åºåˆ—åŒ–
        try:
            # å˜—è©¦åºåˆ—åŒ–æ¸¬è©¦
            import json
            from bs4 import BeautifulSoup
            
            # å¦‚æœæ˜¯ BeautifulSoup ç‰©ä»¶ï¼Œå‰‡ä¸å¿«å–
            if isinstance(data, BeautifulSoup):
                logger.debug(f"ğŸš« BeautifulSoup ç‰©ä»¶ä¸å¯å¿«å–: {url}")
                return
            
            # æ¸¬è©¦æ˜¯å¦å¯ä»¥åºåˆ—åŒ–ç‚º JSON
            json.dumps(data, ensure_ascii=False)
            
        except (TypeError, ValueError) as e:
            logger.debug(f"ğŸš« è³‡æ–™ä¸å¯åºåˆ—åŒ–ï¼Œè·³éå¿«å–: {url} - {e}")
            return
            
        cache_key = self._generate_cache_key(url, params)
        entry = CacheEntry(
            data=data,
            timestamp=time.time(),
            url=url,
            request_hash=cache_key
        )
        
        self.cache[cache_key] = entry
        logger.debug(f"ğŸ’¾ å·²å¿«å–: {url}")

    def safe_request(self, request_func: Callable, url: str, *args, **kwargs) -> Optional[Any]:
        """å®‰å…¨è«‹æ±‚åŒ…è£å™¨ - åŒ…å«é–“éš”æ§åˆ¶ã€å¿«å–å’Œé‡è©¦æ©Ÿåˆ¶"""
        
        # æª¢æŸ¥å¿«å–
        params = kwargs.get('params', {})
        cached_result = self.get_from_cache(url, params)
        if cached_result is not None:
            return cached_result
        
        # æ§åˆ¶è«‹æ±‚é–“éš”
        self._wait_for_next_request()
        
        # è¨­ç½®è«‹æ±‚æ¨™é ­
        if 'headers' not in kwargs:
            kwargs['headers'] = self.get_headers()
        
        # å¯¦æ–½é‡è©¦æ©Ÿåˆ¶
        last_exception = None
        for attempt in range(self.config.max_retries + 1):
            try:
                logger.debug(f"ğŸŒ ç™¼é€è«‹æ±‚ (å˜—è©¦ {attempt + 1}/{self.config.max_retries + 1}): {url}")
                
                result = request_func(url, *args, **kwargs)
                
                # ä¿å­˜åˆ°å¿«å–
                if result is not None:
                    self.save_to_cache(url, result, params)
                    
                return result
                
            except Exception as e:
                last_exception = e
                logger.warning(f"âš ï¸ è«‹æ±‚å¤±æ•— (å˜—è©¦ {attempt + 1}): {e}")
                
                if attempt < self.config.max_retries:
                    # æŒ‡æ•¸é€€é¿å»¶é²
                    wait_time = self.config.backoff_factor ** attempt
                    logger.info(f"â³ ç­‰å¾… {wait_time:.1f} ç§’å¾Œé‡è©¦...")
                    time.sleep(wait_time)
                    
                    # è¼ªæ›¿æ¨™é ­
                    if self.config.rotate_headers:
                        kwargs['headers'] = self.get_headers()
        
        logger.error(f"âŒ æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—äº†: {url}")
        if last_exception:
            raise last_exception
        
        return None

    def __del__(self):
        """ææ§‹å‡½æ•¸ - ä¿å­˜å¿«å–"""
        try:
            self._save_cache()
        except:
            pass

    def get_stats(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        valid_cache_count = sum(1 for entry in self.cache.values() if self._is_cache_valid(entry))
        
        return {
            'config': asdict(self.config),
            'cache_stats': {
                'total_entries': len(self.cache),
                'valid_entries': valid_cache_count,
                'expired_entries': len(self.cache) - valid_cache_count,
                'cache_file': self.cache_file
            },
            'browser_headers_count': len(self.browser_headers),
            'current_header_index': self.current_header_index
        }

    def clear_cache(self):
        """æ¸…ç©ºå¿«å–"""
        self.cache.clear()
        try:
            cache_path = Path(self.cache_file)
            if cache_path.exists():
                cache_path.unlink()
            logger.info("ğŸ§¹ å·²æ¸…ç©ºæ‰€æœ‰å¿«å–")
        except Exception as e:
            logger.warning(f"æ¸…ç©ºå¿«å–æª”æ¡ˆå¤±æ•—: {e}")

    def configure(self, **kwargs):
        """å‹•æ…‹é…ç½®æœå°‹å™¨"""
        for key, value in kwargs.items():
            if hasattr(self.config, key):
                setattr(self.config, key, value)
                logger.info(f"âš™ï¸ å·²æ›´æ–°é…ç½®: {key} = {value}")
            else:
                logger.warning(f"æœªçŸ¥é…ç½®é …ç›®: {key}")