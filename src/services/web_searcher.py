# -*- coding: utf-8 -*-
"""
ç¶²è·¯æœå°‹å™¨æ¨¡çµ„
"""
import re
import time
import logging
import threading
import concurrent.futures
import chardet
from typing import Dict, List, Optional
import httpx
from bs4 import BeautifulSoup
from urllib.parse import quote

import sys
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°ç³»çµ±è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from models.config import ConfigManager
from .safe_searcher import SafeSearcher, RequestConfig
from .safe_javdb_searcher import SafeJAVDBSearcher
# ç§»é™¤ä¸å¿…è¦çš„ create_japanese_soup åŒ¯å…¥ï¼Œç›´æ¥ä½¿ç”¨ JapaneseSiteEnhancer é¡åˆ¥

logger = logging.getLogger(__name__)


class WebSearcher:
    """å¢å¼·ç‰ˆæœå°‹å™¨ - æ”¯æ´æœå°‹çµæœé é¢"""
    
    def __init__(self, config: ConfigManager):
        # åˆå§‹åŒ–å®‰å…¨æœå°‹å™¨é…ç½®
        safe_config = RequestConfig(
            min_interval=config.getfloat('search', 'min_interval', fallback=1.0),
            max_interval=config.getfloat('search', 'max_interval', fallback=3.0),
            enable_cache=config.getboolean('search', 'enable_cache', fallback=True),
            cache_duration=config.getint('search', 'cache_duration', fallback=86400),
            max_retries=config.getint('search', 'max_retries', fallback=3),
            backoff_factor=config.getfloat('search', 'backoff_factor', fallback=2.0),
            rotate_headers=config.getboolean('search', 'rotate_headers', fallback=True)
        )
        
        # åˆå§‹åŒ–æ—¥æ–‡ç¶²ç«™å°ˆç”¨çš„æ›´å¿«é€Ÿé…ç½®ï¼ˆav-wiki å’Œ chiba-f æ¯”è¼ƒä¸æœƒæ“‹çˆ¬èŸ²ï¼‰
        japanese_config = RequestConfig(
            min_interval=config.getfloat('search', 'japanese_min_interval', fallback=0.5),
            max_interval=config.getfloat('search', 'japanese_max_interval', fallback=1.5),
            enable_cache=config.getboolean('search', 'enable_cache', fallback=True),
            cache_duration=config.getint('search', 'cache_duration', fallback=86400),
            max_retries=config.getint('search', 'max_retries', fallback=3),
            backoff_factor=config.getfloat('search', 'backoff_factor', fallback=1.5),
            rotate_headers=config.getboolean('search', 'rotate_headers', fallback=True)
        )
          # åˆå§‹åŒ–å®‰å…¨æœå°‹å™¨
        self.safe_searcher = SafeSearcher(safe_config)
        
        # ç‚ºæ—¥æ–‡ç¶²ç«™å»ºç«‹æ›´å¿«é€Ÿçš„æœå°‹å™¨ï¼ˆav-wiki å’Œ chiba-f æ¯”è¼ƒä¸æœƒæ“‹çˆ¬èŸ²ï¼‰
        japanese_config = RequestConfig(
            min_interval=0.5,  # æ—¥æ–‡ç¶²ç«™è¼ƒçŸ­å»¶é²
            max_interval=1.5,
            enable_cache=True,
            cache_duration=86400,
            max_retries=3,
            backoff_factor=1.5,
            rotate_headers=True
        )
        self.japanese_searcher = SafeSearcher(japanese_config)
        self.japanese_searcher = SafeSearcher(japanese_config)  # æ—¥æ–‡ç¶²ç«™å°ˆç”¨
        
        # åˆå§‹åŒ– JAVDB å®‰å…¨æœå°‹å™¨
        cache_dir = config.get('search', 'cache_dir', fallback=None)
        self.javdb_searcher = SafeJAVDBSearcher(cache_dir)
          # ä¿ç•™åŸæœ‰é…ç½®ä»¥å‘ä¸‹ç›¸å®¹
        self.headers = self.safe_searcher.get_headers()
        
        # ğŸ”§ æ—¥æ–‡ç¶²ç«™å°ˆç”¨æ¨™é ­ï¼ˆè§£æ±º Brotli å£“ç¸®å•é¡Œï¼‰
        self.japanese_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Charset': 'UTF-8,Shift_JIS,EUC-JP,ISO-2022-JP,*;q=0.1',
            'Accept-Encoding': 'identity',  # æ˜ç¢ºæ‹’çµ•æ‰€æœ‰å£“ç¸®
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        self.search_cache = {}
        self.batch_size = config.getint('search', 'batch_size', fallback=10)
        self.thread_count = config.getint('search', 'thread_count', fallback=5)
        self.batch_delay = config.getfloat('search', 'batch_delay', fallback=2.0)
        self.timeout = config.getint('search', 'request_timeout', fallback=20)
        logger.info("ğŸ›¡ï¸ å·²å•Ÿç”¨å®‰å…¨æœå°‹å™¨åŠŸèƒ½")
        logger.info("ğŸ‡¯ğŸ‡µ å·²å•Ÿç”¨æ—¥æ–‡ç¶²ç«™å¿«é€Ÿæœå°‹åŠŸèƒ½")
        logger.info("ğŸ¬ å·²å•Ÿç”¨ JAVDB å®‰å…¨æœå°‹åŠŸèƒ½")

    def search_info(self, code: str, stop_event: threading.Event) -> Optional[Dict]:
        """å¤šå±¤ç´šæœå°‹ç­–ç•¥ - AV-WIKI -> chiba-f.net -> JAVDB"""
        if stop_event.is_set(): 
            return None
        if code in self.search_cache: 
            return self.search_cache[code]
        
        try:
            # ç¬¬ä¸€å±¤ï¼šåŸæœ‰çš„ AV-WIKI æœå°‹
            logger.debug(f"ğŸ” ç¬¬ä¸€å±¤æœå°‹ - AV-WIKI: {code}")
            result = self._search_av_wiki(code, stop_event)
            if result and result.get('actresses'):
                self.search_cache[code] = result
                return result
            
            # ç¬¬äºŒå±¤ï¼šchiba-f.net æœå°‹  
            if not stop_event.is_set():
                logger.debug(f"ğŸ” ç¬¬äºŒå±¤æœå°‹ - chiba-f.net: {code}")
                result = self._search_chiba_f_net(code, stop_event)
                if result and result.get('actresses'):
                    self.search_cache[code] = result
                    return result
            
            # ç¬¬ä¸‰å±¤ï¼šä½¿ç”¨å®‰å…¨çš„ JAVDB æœå°‹
            if not stop_event.is_set():
                logger.debug(f"ğŸ” ç¬¬ä¸‰å±¤æœå°‹ - JAVDB: {code}")
                javdb_result = self.javdb_searcher.search_javdb(code)
                if javdb_result and javdb_result.get('actresses'):
                    # è½‰æ›ç‚ºçµ±ä¸€æ ¼å¼
                    result = {
                        'source': javdb_result['source'],
                        'actresses': javdb_result['actresses'],
                        'studio': javdb_result.get('studio'),
                        'studio_code': javdb_result.get('studio_code'),
                        'release_date': javdb_result.get('release_date'),
                        'title': javdb_result.get('title'),
                        'duration': javdb_result.get('duration'),
                        'director': javdb_result.get('director'),
                        'series': javdb_result.get('series'),
                        'rating': javdb_result.get('rating'),
                        'categories': javdb_result.get('categories', [])
                    }
                    self.search_cache[code] = result
                    
                    # è±å¯Œçš„æ—¥èªŒè¼¸å‡º
                    log_parts = [f"ç•ªè™Ÿ {code} é€é {result['source']} æ‰¾åˆ°:"]
                    log_parts.append(f"å¥³å„ª: {', '.join(result['actresses'])}")
                    log_parts.append(f"ç‰‡å•†: {result.get('studio', 'æœªçŸ¥')}")
                    
                    if result.get('rating'):
                        log_parts.append(f"è©•åˆ†: {result['rating']}")
                    if result.get('categories'):
                        categories_str = ', '.join(result['categories'][:3])  # åªé¡¯ç¤ºå‰3å€‹é¡åˆ¥
                        if len(result['categories']) > 3:
                            categories_str += f" ç­‰{len(result['categories'])}å€‹é¡åˆ¥"
                        log_parts.append(f"é¡åˆ¥: {categories_str}")
                    
                    logger.info(" | ".join(log_parts))
                    return result
            
            logger.warning(f"ç•ªè™Ÿ {code} æœªåœ¨æ‰€æœ‰æœå°‹æºä¸­æ‰¾åˆ°å¥³å„ªè³‡è¨Šã€‚")
            return None            
        except Exception as e:
            logger.error(f"æœå°‹ç•ªè™Ÿ {code} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return None

    def _search_av_wiki(self, code: str, stop_event: threading.Event) -> Optional[Dict]:
        """AV-WIKI æœå°‹æ–¹æ³•"""
        if stop_event.is_set():
            return None
            
        search_url = f"https://av-wiki.net/?s={quote(code)}&post_type=product"
          # ä½¿ç”¨æ—¥æ–‡ç¶²ç«™å°ˆç”¨æ¨™é ­å’Œå¢å¼·ç·¨ç¢¼æª¢æ¸¬
        def make_request(url, **kwargs):
            with httpx.Client(timeout=self.timeout, **kwargs) as client:
                # ğŸ”§ ä½¿ç”¨ä¸æ”¯æ´å£“ç¸®çš„æ¨™é ­ï¼Œé¿å… Brotli å•é¡Œ
                response = client.get(url, headers=self.japanese_headers)
                response.raise_for_status()
                # ğŸ”§ ä½¿ç”¨å¢å¼·çš„ç·¨ç¢¼æª¢æ¸¬æ©Ÿåˆ¶
                decoded_content = self._detect_and_decode_content(response)
                logger.debug(f"ğŸ“„ AV-WIKI å…§å®¹é•·åº¦: {len(decoded_content)} å­—ç¬¦")
                logger.debug(f"ğŸ“„ AV-WIKI å…§å®¹é–‹é ­: {decoded_content[:100]}...")
                return BeautifulSoup(decoded_content, 'html.parser')
        
        try:
            soup = self.safe_searcher.safe_request(make_request, search_url)
            
            if soup is None:
                logger.warning(f"ç„¡æ³•ç²å– {code} çš„ AV-WIKI æœå°‹é é¢")
                return None
            
            # å…ˆæª¢æŸ¥æ˜¯å¦æœ‰æœå°‹çµæœ
            search_results = soup.find_all("div", class_="column-flex")
            logger.info(f"AV-WIKI æœå°‹ {code}: æ‰¾åˆ° {len(search_results)} å€‹æœå°‹çµæœ")
            
            if not search_results:
                # æª¢æŸ¥æ˜¯å¦æ˜¯ "æ²’æœ‰æ‰¾åˆ°çµæœ" çš„é é¢
                no_results_indicators = ["è©²å½“ãªã—", "è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ", "æ¤œç´¢çµæœï¼š0", "0ä»¶"]
                page_text = soup.get_text()
                for indicator in no_results_indicators:
                    if indicator in page_text:
                        logger.info(f"AV-WIKI æ˜ç¢ºé¡¯ç¤ºæ²’æœ‰æ‰¾åˆ° {code} çš„çµæœ")
                        return None
                        
            # æ­£ç¢ºè§£æå¥³å„ªåç¨±ï¼š<li class="actress-name"><a>å¥³å„ªåç¨±</a></li>
            actress_elements = soup.find_all("li", class_="actress-name")
            actresses = []
            logger.info(f"AV-WIKI è§£æ: æ‰¾åˆ° {len(actress_elements)} å€‹ actress-name å…ƒç´ ")
            for li in actress_elements:
                link = li.find("a")
                if link and link.text.strip():
                    actress_name = link.text.strip()
                    actresses.append(actress_name)
                    logger.info(f"AV-WIKI æå–åˆ°å¥³å„ªåç¨±: {actress_name}")
            
            if not actresses:
                logger.warning(f"AV-WIKI æœªæ‰¾åˆ°å¥³å„ªåç¨±ï¼ŒHTMLé–‹é ­: {str(soup)[:200]}...")
            
            # æœå°‹ç‰‡å•†è³‡è¨Š
            studio_info = self._extract_studio_info(soup, code)
            
            if not actresses:
                page_text = soup.get_text()
                lines = [line.strip() for line in page_text.split('\n') if line.strip()]
                for i, line in enumerate(lines):
                    if code in line:
                        for j in range(max(0, i-3), min(len(lines), i+1)):
                            potential_name = lines[j].strip()
                            if potential_name and self._is_actress_name(potential_name):
                                if potential_name not in actresses: 
                                    actresses.append(potential_name)                
            if actresses:
                result = {
                    'source': 'AV-WIKI (å®‰å…¨å¢å¼·ç‰ˆ)', 
                    'actresses': actresses,
                    'studio': studio_info.get('studio'),
                    'studio_code': studio_info.get('studio_code'),
                    'release_date': studio_info.get('release_date')
                }
                logger.info(f"ç•ªè™Ÿ {code} é€é {result['source']} æ‰¾åˆ°: {', '.join(result['actresses'])}, ç‰‡å•†: {result.get('studio', 'æœªçŸ¥')}")
                return result

        except Exception as e:
            logger.error(f"AV-WIKI æœå°‹ {code} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        
        return None

    def _is_actress_name(self, text: str) -> bool:
        """åˆ¤æ–·æ–‡å­—æ˜¯å¦å¯èƒ½æ˜¯å¥³å„ªåç¨±"""
        if not text or len(text) < 2 or len(text) > 20: 
            return False
        exclude_keywords = [
            'SOD', 'STARS', 'FANZA', 'MGS', 'MIDV', 'SSIS', 'IPX', 'IPZZ', 
            'ç¶šãã‚’èª­ã‚€', 'æ¤œç´¢', 'ä»¶', 'ç‰¹å…¸', 'æ˜ åƒ', 'ä»˜ã', 'star', 'SOKMIL', 
            'Menu', 'ã‚»ãƒ¼ãƒ«', 'é™å®š', 'æœ€å¤§'
        ]
        if any(keyword in text for keyword in exclude_keywords): 
            return False
        if re.match(r'^\d+$', text) or len(re.findall(r'\d', text)) > len(text) // 2: 
            return False
        if re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', text): 
            return True
        return False
    
    def _detect_and_decode_content(self, response: httpx.Response) -> str:
        """å¤šé‡ç·¨ç¢¼æª¢æ¸¬å’Œè§£ç¢¼æ©Ÿåˆ¶ï¼ˆæ”¯æ´å£“ç¸®å…§å®¹è™•ç†ï¼‰"""
        content_bytes = response.content
        
        # ğŸ”§ é¦–å…ˆæª¢æŸ¥æ˜¯å¦ç‚ºå£“ç¸®å…§å®¹
        content_bytes = self._handle_compression(response, content_bytes)
        
        # æª¢æŸ¥å…§å®¹æ˜¯å¦çœ‹èµ·ä¾†åƒäºŒé€²åˆ¶å£“ç¸®æ•¸æ“š
        if self._is_likely_compressed(content_bytes):
            logger.warning("âš ï¸ å…§å®¹ä»ç„¶çœ‹èµ·ä¾†åƒå£“ç¸®æ•¸æ“šï¼Œå˜—è©¦å¼·åˆ¶è§£å£“")
            content_bytes = self._force_decompress(content_bytes)
        
        # å¦‚æœæª¢æ¸¬åˆ° brotli ä½†æ²’æœ‰åº«ï¼Œå¼·åˆ¶å˜—è©¦è§£å£“
        content_encoding = response.headers.get('content-encoding', '').lower()
        if content_encoding == 'br' and len(content_bytes) > 0:
            logger.warning("âš ï¸ æœå‹™å™¨ç™¼é€äº† brotli å£“ç¸®å…§å®¹ï¼Œå˜—è©¦å¼·åˆ¶è§£å£“")
            content_bytes = self._force_decompress(content_bytes)
        
        # å„ªå…ˆé †åºï¼šUTF-8 > Shift_JIS > EUC-JP > CP932 > è‡ªå‹•æª¢æ¸¬
        encoding_attempts = ['utf-8', 'shift_jis', 'euc-jp', 'cp932', 'iso-2022-jp']
        
        # å¦‚æœ response å·²ç¶“æœ‰ç·¨ç¢¼ä¿¡æ¯ï¼Œå„ªå…ˆå˜—è©¦
        if response.encoding and response.encoding.lower() not in [enc.lower() for enc in encoding_attempts]:
            encoding_attempts.insert(0, response.encoding.lower())
        
        # å˜—è©¦æ¯ç¨®ç·¨ç¢¼
        for encoding in encoding_attempts:
            try:
                decoded_text = content_bytes.decode(encoding)
                # é©—è­‰è§£ç¢¼æ˜¯å¦æˆåŠŸï¼ˆæª¢æŸ¥æ˜¯å¦æœ‰æ˜é¡¯çš„äº‚ç¢¼ï¼‰
                if self._is_valid_decoded_text(decoded_text):
                    logger.debug(f"âœ… æˆåŠŸä½¿ç”¨ç·¨ç¢¼ {encoding} è§£ç¢¼å…§å®¹")
                    return decoded_text
            except (UnicodeDecodeError, LookupError):
                logger.debug(f"âŒ ç·¨ç¢¼ {encoding} è§£ç¢¼å¤±æ•—")
                continue
        
        # å¦‚æœæ‰€æœ‰é è¨­ç·¨ç¢¼éƒ½å¤±æ•—ï¼Œä½¿ç”¨ chardet è‡ªå‹•æª¢æ¸¬
        try:
            detected = chardet.detect(content_bytes[:10000])  # åªæª¢æ¸¬å‰10Kå­—ç¯€ä»¥æé«˜æ•ˆç‡
            if detected and detected['encoding'] and detected['confidence'] > 0.6:
                encoding = detected['encoding']
                decoded_text = content_bytes.decode(encoding)
                if self._is_valid_decoded_text(decoded_text):
                    logger.info(f"ğŸ” é€šéè‡ªå‹•æª¢æ¸¬ä½¿ç”¨ç·¨ç¢¼ {encoding} (ç½®ä¿¡åº¦: {detected['confidence']:.2f})")
                    return decoded_text
        except Exception as e:
            logger.warning(f"è‡ªå‹•ç·¨ç¢¼æª¢æ¸¬å¤±æ•—: {e}")
        
        # æœ€å¾Œæ‰‹æ®µï¼šä½¿ç”¨ errors='replace' å¼·åˆ¶è§£ç¢¼
        logger.warning("æ‰€æœ‰ç·¨ç¢¼å˜—è©¦å¤±æ•—ï¼Œä½¿ç”¨ UTF-8 å¼·åˆ¶è§£ç¢¼")
        return content_bytes.decode('utf-8', errors='replace')
    
    def _handle_compression(self, response: httpx.Response, content_bytes: bytes) -> bytes:
        """è™•ç†HTTPå£“ç¸®å…§å®¹"""
        import gzip
        import zlib
        
        # å˜—è©¦å°å…¥ brotliï¼ˆå¯é¸ï¼‰
        try:
            import brotli
            brotli_available = True
        except ImportError:
            logger.debug("âŒ brotli åº«æœªå®‰è£ï¼Œè·³é brotli è§£å£“ç¸®")
            brotli_available = False
        
        # æª¢æŸ¥ Content-Encoding æ¨™é ­
        content_encoding = response.headers.get('content-encoding', '').lower()
        
        try:
            if content_encoding == 'gzip':
                logger.debug("ğŸ”§ æª¢æ¸¬åˆ° gzip å£“ç¸®ï¼Œæ­£åœ¨è§£å£“")
                return gzip.decompress(content_bytes)
            elif content_encoding == 'br' and brotli_available:
                logger.debug("ğŸ”§ æª¢æ¸¬åˆ° brotli å£“ç¸®ï¼Œæ­£åœ¨è§£å£“")
                return brotli.decompress(content_bytes)
            elif content_encoding == 'br' and not brotli_available:
                logger.warning("âš ï¸ æª¢æ¸¬åˆ° brotli å£“ç¸®ä½†æœªå®‰è£ brotli åº«ï¼Œå˜—è©¦å…¶ä»–æ–¹æ³•")
            elif content_encoding == 'deflate':
                logger.debug("ğŸ”§ æª¢æ¸¬åˆ° deflate å£“ç¸®ï¼Œæ­£åœ¨è§£å£“")
                return zlib.decompress(content_bytes)
        except Exception as e:
            logger.warning(f"âš ï¸ å£“ç¸®è§£ç¢¼å¤±æ•—: {e}")
        
        return content_bytes
    
    def _is_likely_compressed(self, content_bytes: bytes) -> bool:
        """æª¢æŸ¥å…§å®¹æ˜¯å¦çœ‹èµ·ä¾†åƒå£“ç¸®æ•¸æ“š"""
        if len(content_bytes) < 10:
            return False
        
        # æª¢æŸ¥å¸¸è¦‹å£“ç¸®æ ¼å¼çš„é­”è¡“å­—ç¯€
        # gzip: 1f 8b
        # brotli: é€šå¸¸æœ‰é«˜ç†µå€¼
        # deflate: 78 9c, 78 01, 78 da, 78 5e
        
        first_bytes = content_bytes[:10]
        
        # gzip é­”è¡“å­—ç¯€
        if first_bytes.startswith(b'\x1f\x8b'):
            return True
        
        # deflate é­”è¡“å­—ç¯€
        if first_bytes.startswith((b'\x78\x9c', b'\x78\x01', b'\x78\xda', b'\x78\x5e')):
            return True
        
        # æª¢æŸ¥æ˜¯å¦æœ‰éå¤šçš„éASCIIå­—ç¬¦ï¼ˆå¯èƒ½æ˜¯å£“ç¸®æ•¸æ“šï¼‰
        non_ascii_count = sum(1 for b in first_bytes if b > 127)
        if non_ascii_count > len(first_bytes) * 0.5:
            return True
        
        return False
    
    def _force_decompress(self, content_bytes: bytes) -> bytes:
        """å¼·åˆ¶å˜—è©¦æ‰€æœ‰å¯èƒ½çš„è§£å£“æ–¹æ³•"""
        import gzip
        import zlib
        
        # å˜—è©¦å°å…¥ brotliï¼ˆå¯é¸ï¼‰
        try:
            import brotli
            brotli_available = True
        except ImportError:
            brotli_available = False
        
        decompress_methods = [
            ("gzip", gzip.decompress),
            ("deflate", zlib.decompress),
            ("deflate (raw)", lambda x: zlib.decompress(x, -15))  # raw deflate
        ]
        
        # å¦‚æœ brotli å¯ç”¨ï¼Œæ·»åŠ åˆ°è§£å£“æ–¹æ³•åˆ—è¡¨
        if brotli_available:
            decompress_methods.insert(1, ("brotli", brotli.decompress))
        
        for method_name, decompress_func in decompress_methods:
            try:
                decompressed = decompress_func(content_bytes)
                logger.info(f"ğŸ‰ æˆåŠŸä½¿ç”¨ {method_name} è§£å£“ç¸®")
                return decompressed
            except Exception as e:
                logger.debug(f"âŒ {method_name} è§£å£“å¤±æ•—: {e}")
                continue
        
        logger.warning("âš ï¸ æ‰€æœ‰è§£å£“æ–¹æ³•éƒ½å¤±æ•—ï¼Œè¿”å›åŸå§‹å…§å®¹")
        return content_bytes
    
    def _is_valid_decoded_text(self, text: str) -> bool:
        """é©—è­‰è§£ç¢¼å¾Œçš„æ–‡å­—æ˜¯å¦æœ‰æ•ˆï¼ˆç„¡æ˜é¡¯äº‚ç¢¼ï¼‰"""
        if not text or len(text) < 10:
            return False
        
        # æª¢æŸ¥æ˜¯å¦æœ‰éå¤šçš„æ›¿æ›å­—ç¬¦ï¼ˆï¿½ï¼‰
        replacement_ratio = text.count('ï¿½') / len(text)
        if replacement_ratio > 0.1:  # æ”¾å¯¬åˆ°10%ï¼Œå› ç‚ºæœ‰äº›ç¶²ç«™å¯èƒ½åŒ…å«ç‰¹æ®Šå­—ç¬¦
            logger.debug(f"âŒ æ›¿æ›å­—ç¬¦æ¯”ä¾‹éé«˜: {replacement_ratio:.2%}")
            return False
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«åŸºæœ¬çš„HTMLæ¨™ç±¤
        html_tags = ['<html', '<body', '<div', '<span', '<a', '<title', '<head', '<!doctype']
        has_html = any(tag in text.lower() for tag in html_tags)
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æ—¥æ–‡å­—ç¬¦
        has_japanese = re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', text)
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«å¸¸è¦‹çš„HTMLå¯¦é«”æˆ–æ¨™æº–å­—ç¬¦
        has_entities = any(entity in text for entity in ['&lt;', '&gt;', '&amp;', '&quot;'])
        
        # æª¢æŸ¥å­—ç¬¦åˆ†ä½ˆæ˜¯å¦åˆç†ï¼ˆä¸æ˜¯ç´”äºŒé€²åˆ¶æ•¸æ“šï¼‰
        printable_chars = sum(1 for c in text[:1000] if c.isprintable() or c.isspace())
        printable_ratio = printable_chars / min(len(text), 1000)
        
        is_valid = (has_html or has_japanese or has_entities) and printable_ratio > 0.7
        
        if not is_valid:
            logger.debug(f"âŒ å…§å®¹é©—è­‰å¤±æ•— - HTML:{has_html} æ—¥æ–‡:{bool(has_japanese)} å¯¦é«”:{has_entities} å¯åˆ—å°æ¯”ä¾‹:{printable_ratio:.2%}")
        
        return is_valid

    def batch_search(self, items: List, task_func, stop_event: threading.Event, progress_callback=None) -> Dict:
        results = {}
        total_batches = (len(items) + self.batch_size - 1) // self.batch_size
        for i in range(0, len(items), self.batch_size):
            if stop_event.is_set(): 
                logger.info("ä»»å‹™è¢«ä½¿ç”¨è€…ä¸­æ­¢ã€‚")
                break
            batch = items[i:i + self.batch_size]
            batch_num = (i // self.batch_size) + 1
            if progress_callback: 
                progress_callback(f"è™•ç†æ‰¹æ¬¡ {batch_num}/{total_batches}...\n")
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.thread_count) as executor:
                future_to_item = {executor.submit(task_func, item, stop_event): item for item in batch}
                for future in concurrent.futures.as_completed(future_to_item):
                    if stop_event.is_set(): 
                        break
                    item = future_to_item[future]
                    try:
                        result = future.result()
                        results[item] = result
                        if progress_callback:
                            if result and result.get('actresses'): 
                                progress_callback(f"âœ… {item}: æ‰¾åˆ°è³‡æ–™\n")
                            else: 
                                progress_callback(f"âŒ {item}: æœªæ‰¾åˆ°çµæœ\n")
                    except Exception as e:
                        logger.error(f"æ‰¹æ¬¡è™•ç† {item} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        if progress_callback: 
                            progress_callback(f"ğŸ’¥ {item}: è™•ç†å¤±æ•— - {e}\n")
            if i + self.batch_size < len(items) and total_batches > 1:                time.sleep(self.batch_delay)
        return results
    
    def _search_chiba_f_net(self, code: str, stop_event: threading.Event) -> Optional[Dict]:
        """ä½¿ç”¨ chiba-f.net æœå°‹å¥³å„ªè³‡è¨Š"""
        if stop_event.is_set():
            return None
            
        search_url = f"https://chiba-f.net/search/?keyword={quote(code)}"
          # ä½¿ç”¨æ—¥æ–‡ç¶²ç«™å°ˆç”¨æ¨™é ­å’Œå¢å¼·ç·¨ç¢¼æª¢æ¸¬
        def make_request(url, **kwargs):
            with httpx.Client(timeout=self.timeout, **kwargs) as client:
                # ğŸ”§ ä½¿ç”¨ä¸æ”¯æ´å£“ç¸®çš„æ¨™é ­ï¼Œé¿å… Brotli å•é¡Œ
                response = client.get(url, headers=self.japanese_headers)
                response.raise_for_status()
                # ğŸ”§ ä½¿ç”¨å¢å¼·çš„ç·¨ç¢¼æª¢æ¸¬æ©Ÿåˆ¶
                decoded_content = self._detect_and_decode_content(response)
                logger.debug(f"ğŸ“„ chiba-f.net å…§å®¹é•·åº¦: {len(decoded_content)} å­—ç¬¦")
                logger.debug(f"ğŸ“„ chiba-f.net å…§å®¹é–‹é ­: {decoded_content[:100]}...")
                return BeautifulSoup(decoded_content, 'html.parser')
        
        try:
            soup = self.safe_searcher.safe_request(make_request, search_url)
            
            if soup is None:
                logger.warning(f"ç„¡æ³•ç²å– {code} çš„ chiba-f.net æœå°‹é é¢")
                return None
                
            # æŸ¥æ‰¾ç”¢å“å€å¡Š
            product_divs = soup.find_all('div', class_='product-div')
            logger.info(f"chiba-f.net è§£æ: æ‰¾åˆ° {len(product_divs)} å€‹ product-div å…ƒç´ ")
                
            for product_div in product_divs:
                # æª¢æŸ¥ç•ªè™Ÿæ˜¯å¦åŒ¹é…
                pno_element = product_div.find('div', class_='pno')
                if pno_element and code.upper() in pno_element.text.upper():
                    logger.info(f"chiba-f.net æ‰¾åˆ°åŒ¹é…ç•ªè™Ÿ: {code}")
                    return self._extract_chiba_product_info(product_div, code)
            
            # å¦‚æœæ²’æœ‰æ‰¾åˆ°å®Œå…¨åŒ¹é…ï¼Œå˜—è©¦æ¨¡ç³ŠåŒ¹é…
            for product_div in product_divs:
                product_text = product_div.get_text()
                if code.upper() in product_text.upper():
                    logger.info(f"chiba-f.net æ¨¡ç³ŠåŒ¹é…æ‰¾åˆ°ç•ªè™Ÿ: {code}")
                    return self._extract_chiba_product_info(product_div, code)
            
            if not product_divs:
                logger.warning(f"chiba-f.net æœªæ‰¾åˆ°ä»»ä½•ç”¢å“å€å¡Šï¼ŒHTMLé–‹é ­: {str(soup)[:200]}...")
                        
        except Exception as e:
            logger.error(f"chiba-f.net æœå°‹ {code} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            
        logger.debug(f"ç•ªè™Ÿ {code} æœªåœ¨ chiba-f.net ä¸­æ‰¾åˆ°å¥³å„ªè³‡è¨Šã€‚")
        return None
    
    def _extract_chiba_product_info(self, product_div, code: str) -> Dict:
        """å¾ chiba-f.net ç”¢å“å€å¡Šæå–è³‡è¨Š"""
        result = {
            'source': 'chiba-f.net (å®‰å…¨å¢å¼·ç‰ˆ)',
            'actresses': [],
            'studio': None,
            'studio_code': None,
            'release_date': None
        }
        
        try:
            # æå–å¥³å„ªåç¨±
            actress_span = product_div.find('span', class_='fw-bold')
            if actress_span:
                result['actresses'] = [actress_span.text.strip()]
            
            # æå–ç³»åˆ—/ç‰‡å•†è³‡è¨Š
            series_link = product_div.find('a', href=re.compile(r'../series/'))
            if series_link:
                result['studio'] = series_link.text.strip()
                # å¾ href æå–ç‰‡å•†ä»£ç¢¼
                href = series_link.get('href', '')
                if '../series/' in href:
                    result['studio_code'] = href.replace('../series/', '').strip()
            
            # æå–ç™¼è¡Œæ—¥æœŸ
            date_span = product_div.find('span', class_='start_date')
            if date_span:
                result['release_date'] = date_span.text.strip()
            
            # å¦‚æœæ²’æœ‰æ‰¾åˆ°ç‰‡å•†ï¼Œå˜—è©¦å¾ç•ªè™Ÿæ¨æ¸¬
            if not result['studio_code']:
                result['studio_code'] = self._extract_studio_code_from_number(code)
            
            if result['actresses']:
                self.search_cache[code] = result
                logger.info(f"ç•ªè™Ÿ {code} é€é {result['source']} æ‰¾åˆ°: {', '.join(result['actresses'])}, ç‰‡å•†: {result.get('studio', 'æœªçŸ¥')}")
                
        except Exception as e:
            logger.warning(f"æå– {code} å¾ chiba-f.net è³‡è¨Šæ™‚ç™¼ç”Ÿéƒ¨åˆ†éŒ¯èª¤: {str(e)}")
        
        return result if result.get('actresses') else None
    
    def _extract_studio_info(self, soup: BeautifulSoup, code: str) -> Dict:
        """å¾ç¶²é ä¸­æå–ç‰‡å•†è³‡è¨Š"""
        studio_info = {
            'studio': None,
            'studio_code': None,
            'release_date': None
        }
        
        try:
            # å…ˆå–å¾—ç¶²é æ–‡å­—å…§å®¹ï¼Œå¾ŒçºŒæ–¹æ³•éƒ½å¯èƒ½ç”¨åˆ°
            page_text = soup.get_text()
            
            # æ–¹æ³•1: å¾ AV-WIKI HTML çµæ§‹ä¸­ç›´æ¥æå–ç‰‡å•†è³‡è¨Š
            # æŸ¥æ‰¾åŒ…å« fa-clone åœ–æ¨™çš„ li å…ƒç´ 
            studio_elements = soup.find_all("li")
            for li in studio_elements:
                icon = li.find("i", class_="fa-clone")
                if icon:
                    link = li.find("a")
                    if link and link.text.strip():
                        studio_text = link.text.strip()
                        # è§£æç‰‡å•†åç¨±ï¼Œä¾‹å¦‚ "ã‚¨ã‚¹ãƒ¯ãƒ³ - SONE" -> studio="ã‚¨ã‚¹ãƒ¯ãƒ³", code="SONE"
                        if " - " in studio_text:
                            parts = studio_text.split(" - ")
                            studio_info['studio'] = parts[0].strip()
                            studio_info['studio_code'] = parts[1].strip()
                        else:
                            studio_info['studio'] = studio_text
                        break
            
            # æ–¹æ³•2: å¦‚æœæ–¹æ³•1å¤±æ•—ï¼Œå˜—è©¦å¾ç•ªè™Ÿä¸­æå–ç‰‡å•†ä»£ç¢¼
            if not studio_info['studio']:
                studio_code = self._extract_studio_code_from_number(code)
                if studio_code:
                    studio_info['studio_code'] = studio_code
                    studio_info['studio'] = self._get_studio_name_by_code(studio_code)
            
            # æ–¹æ³•3: å¾ç¶²é å…§å®¹ä¸­æœå°‹ç‰‡å•†è³‡è¨Šï¼ˆæœ€å¾Œæ‰‹æ®µï¼‰
            if not studio_info['studio']:
                # æœå°‹å¸¸è¦‹çš„ç‰‡å•†åç¨±å’Œæ¨¡å¼
                studio_patterns = [
                # ç›´æ¥ç‰‡å•†åç¨±åŒ¹é…
                (r'(S1|SOD|MOODYZ|PREMIUM|WANZ|FALENO|ATTACKERS|E-BODY|KAWAII|FITCH|MADONNA|PRESTIGE)', r'\1'),
                # è£½ä½œå…¬å¸/ç™¼è¡Œå•†æ¨¡å¼
                (r'è£½ä½œ[ï¼š:]\s*([^\n\r]+)', r'\1'),
                (r'ç™¼è¡Œ[ï¼š:]\s*([^\n\r]+)', r'\1'),
                (r'ãƒ¡ãƒ¼ã‚«ãƒ¼[ï¼š:]\s*([^\n\r]+)', r'\1'),
                # ç•ªè™Ÿå‰ç¶´æ¨¡å¼ 
                (r'å“ç•ª[ï¼š:]\s*([A-Z]+)-?\d+', r'\1'),
            ]
            
                for pattern, replacement in studio_patterns:
                    match = re.search(pattern, page_text, re.IGNORECASE)
                    if match:
                        extracted_studio = match.group(1).strip()
                        if extracted_studio and len(extracted_studio) < 50:  # åˆç†é•·åº¦é™åˆ¶
                            if not studio_info['studio']:
                                studio_info['studio'] = extracted_studio
                            if not studio_info['studio_code'] and len(extracted_studio) <= 10:
                                studio_info['studio_code'] = extracted_studio
                            break
            
            # æ–¹æ³•4: å˜—è©¦æå–ç™¼è¡Œæ—¥æœŸï¼ˆç¸½æ˜¯åŸ·è¡Œï¼‰
            date_patterns = [
                r'ç™¼å”®æ—¥[ï¼š:]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                r'(\d{4}\.\d{1,2}\.\d{1,2})'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, page_text)
                if match:
                    studio_info['release_date'] = match.group(1)
                    break                    
        except Exception as e:
            logger.warning(f"æå–ç‰‡å•†è³‡è¨Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        return studio_info
    
    def _extract_studio_code_from_number(self, code: str) -> Optional[str]:
        """å¾ç•ªè™Ÿä¸­æå–ç‰‡å•†ä»£ç¢¼"""
        if not code:
            return None
            
        # æå–å­—æ¯éƒ¨åˆ†ä½œç‚ºç‰‡å•†ä»£ç¢¼
        match = re.match(r'^([A-Z]+)', code.upper())
        if match:
            return match.group(1)
        return None
    
    def _get_studio_name_by_code(self, studio_code: str) -> Optional[str]:
        """æ ¹æ“šç‰‡å•†ä»£ç¢¼ç²å–ç‰‡å•†åç¨±ï¼ˆå¾ studios.json è¼‰å…¥ï¼‰"""
        try:
            import json
            from pathlib import Path
            
            # è¼‰å…¥ studios.json æª”æ¡ˆ
            studios_file = Path(__file__).parent.parent.parent / 'studios.json'
            if studios_file.exists():
                with open(studios_file, 'r', encoding='utf-8') as f:
                    studios_data = json.load(f)
                
                # åå‘æŸ¥æ‰¾ï¼šå¾ä»£ç¢¼æ‰¾åˆ°ç‰‡å•†
                studio_code_upper = studio_code.upper()
                for studio_name, codes in studios_data.items():
                    if studio_code_upper in [code.upper() for code in codes]:
                        return studio_name
        except Exception as e:
            logger.warning(f"è¼‰å…¥ studios.json å¤±æ•—: {e}")
        
        # å›é€€åˆ°å…§å»ºå°æ‡‰è¡¨
        studio_mapping = {
            'STAR': 'SOD',
            'STARS': 'SOD', 
            'SDJS': 'SOD',
            'SSIS': 'S1',
            'SSNI': 'S1',
            'IPX': 'IdeaPocket',
            'IPZZ': 'IdeaPocket',
            'MIDV': 'MOODYZ',
            'MIAA': 'MOODYZ',
            'WANZ': 'WANZ FACTORY',
            'FSDSS': 'FALENO',
            'PRED': 'PREMIUM',
            'ABW': 'Prestige',
            'BF': 'BeFree',
            'CAWD': 'kawaii',
            'JUFD': 'Fitch',
            'JUL': 'MADONNA',
            'JUY': 'MADONNA',
        }
        
        return studio_mapping.get(studio_code.upper(), studio_code)
    
    def get_safe_searcher_stats(self) -> Dict:
        """ç²å–å®‰å…¨æœå°‹å™¨çµ±è¨ˆè³‡è¨Š"""
        return self.safe_searcher.get_stats()
    
    def clear_cache(self):
        """æ¸…ç©ºå¿«å–"""
        self.search_cache.clear()
        self.safe_searcher.clear_cache()
        logger.info("ğŸ§¹ å·²æ¸…ç©ºæ‰€æœ‰æœå°‹å¿«å–")
    
    def configure_safe_searcher(self, **kwargs):
        """å‹•æ…‹é…ç½®å®‰å…¨æœå°‹å™¨"""
        self.safe_searcher.configure(**kwargs)
        # æ›´æ–°æœ¬åœ°æ¨™é ­
        self.headers = self.safe_searcher.get_headers()
    
    def get_javdb_stats(self) -> Dict:
        """ç²å– JAVDB æœå°‹å™¨çµ±è¨ˆè³‡è¨Š"""
        return self.javdb_searcher.get_stats()
    
    def get_all_search_stats(self) -> Dict:
        """ç²å–æ‰€æœ‰æœå°‹å™¨çš„çµ±è¨ˆè³‡è¨Š"""
        return {
            'safe_searcher': self.get_safe_searcher_stats(),
            'javdb_searcher': self.get_javdb_stats(),
            'local_cache_entries': len(self.search_cache)
        }
    
    def clear_all_cache(self):
        """æ¸…ç©ºæ‰€æœ‰æœå°‹å¿«å–"""
        self.search_cache.clear()
        self.safe_searcher.clear_cache()
        self.javdb_searcher.clear_cache()
        logger.info("ğŸ§¹ å·²æ¸…ç©ºæ‰€æœ‰æœå°‹å¿«å– (åŒ…å« JAVDB)")
    
    def search_japanese_sites_only(self, code: str, stop_event: threading.Event) -> Optional[Dict]:
        """åƒ…æœå°‹æ—¥æ–‡ç¶²ç«™ - AV-WIKI å’Œ chiba-f.net"""
        if stop_event.is_set(): 
            return None
        if code in self.search_cache: 
            return self.search_cache[code]
        
        try:
            # ç¬¬ä¸€å±¤ï¼šAV-WIKI æœå°‹
            logger.debug(f"ğŸ‡¯ğŸ‡µ æ—¥æ–‡ç¶²ç«™æœå°‹ - AV-WIKI: {code}")
            result = self._search_av_wiki(code, stop_event)
            if result and result.get('actresses'):
                self.search_cache[code] = result
                return result
            
            # ç¬¬äºŒå±¤ï¼šchiba-f.net æœå°‹
            if not stop_event.is_set():
                logger.debug(f"ğŸ‡¯ğŸ‡µ æ—¥æ–‡ç¶²ç«™æœå°‹ - chiba-f.net: {code}")
                result = self._search_chiba_f_net(code, stop_event)
                if result and result.get('actresses'):
                    self.search_cache[code] = result
                    return result
            
            logger.debug(f"ğŸ‡¯ğŸ‡µ æ—¥æ–‡ç¶²ç«™æœªæ‰¾åˆ°: {code}")
            return None
            
        except Exception as e:
            logger.error(f"æœå°‹ {code} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return None
    
    def search_javdb_only(self, code: str, stop_event: threading.Event) -> Optional[Dict]:
        """åƒ…æœå°‹ JAVDB"""
        if stop_event.is_set(): 
            return None
        if code in self.search_cache: 
            return self.search_cache[code]
        
        try:
            logger.debug(f"ğŸ“Š JAVDB æœå°‹: {code}")
            javdb_result = self.javdb_searcher.search_javdb(code)
            if javdb_result and javdb_result.get('actresses'):
                # è½‰æ›ç‚ºçµ±ä¸€æ ¼å¼
                result = {
                    'source': javdb_result['source'],
                    'actresses': javdb_result['actresses'],
                    'studio': javdb_result.get('studio'),
                    'studio_code': javdb_result.get('studio_code'),
                    'release_date': javdb_result.get('release_date'),
                    'title': javdb_result.get('title'),
                    'duration': javdb_result.get('duration'),
                    'director': javdb_result.get('director'),
                    'series': javdb_result.get('series'),
                    'rating': javdb_result.get('rating'),
                    'categories': javdb_result.get('categories', [])
                }
                self.search_cache[code] = result
                return result
            
            logger.debug(f"ğŸ“Š JAVDB æœªæ‰¾åˆ°: {code}")
            return None
            
        except Exception as e:
            logger.error(f"JAVDB æœå°‹ {code} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return None

    def search_japanese_sites(self, code: str, stop_event: threading.Event) -> Optional[Dict]:
        """åªæœå°‹æ—¥æ–‡ç¶²ç«™ (AV-WIKI å’Œ chiba-f.net)"""
        if stop_event.is_set(): 
            return None
        if code in self.search_cache: 
            return self.search_cache[code]
        
        try:
            # ç¬¬ä¸€å±¤ï¼šAV-WIKI æœå°‹
            logger.debug(f"ğŸ‡¯ğŸ‡µ æ—¥æ–‡ç¶²ç«™æœå°‹ - AV-WIKI: {code}")
            result = self._search_av_wiki(code, stop_event)
            if result and result.get('actresses'):
                self.search_cache[code] = result
                return result
            
            # ç¬¬äºŒå±¤ï¼šchiba-f.net æœå°‹  
            if not stop_event.is_set():
                logger.debug(f"ğŸ‡¯ğŸ‡µ æ—¥æ–‡ç¶²ç«™æœå°‹ - chiba-f.net: {code}")
                result = self._search_chiba_f_net(code, stop_event)
                if result and result.get('actresses'):
                    self.search_cache[code] = result
                    return result
              
            logger.warning(f"ç•ªè™Ÿ {code} æœªåœ¨æ—¥æ–‡ç¶²ç«™ä¸­æ‰¾åˆ°å¥³å„ªè³‡è¨Šã€‚")
            return None            
        except Exception as e:
            logger.error(f"æ—¥æ–‡ç¶²ç«™æœå°‹ç•ªè™Ÿ {code} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return None

